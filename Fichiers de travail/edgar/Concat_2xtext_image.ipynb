{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  DEEPLEARNING IMAGES & TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Edgar\\\\Documents\\\\Rakuten\\\\images\\\\image_train'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import os #Miscellaneous operating system interfaces\n",
    "#https://docs.python.org/3/library/os.html\n",
    "#get current working directory\n",
    "path = os.getcwd() + '\\\\images\\\\image_train'\n",
    "path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NON TRAITÉ (RNN_v5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Données textuelles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(r'C:\\Users\\Edgar\\Documents\\Rakuten\\X_train_update.csv',index_col =0)\n",
    "y = pd.read_csv(r'C:\\Users\\Edgar\\Documents\\Rakuten\\Y_train_CVw08PX.csv',index_col=0).squeeze().map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "      <td></td>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...</td>\n",
       "      <td></td>\n",
       "      <td>436067568</td>\n",
       "      <td>1008141237</td>\n",
       "      <td>Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n",
       "      <td>201115110</td>\n",
       "      <td>938777978</td>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         designation  \\\n",
       "0  Olivia: Personalisiertes Notizbuch / 150 Seite...   \n",
       "1  Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...   \n",
       "2  Grand Stylet Ergonomique Bleu Gamepad Nintendo...   \n",
       "\n",
       "                                         description   productid     imageid  \\\n",
       "0                                                     3804725264  1263597046   \n",
       "1                                                      436067568  1008141237   \n",
       "2  PILOT STYLE Touch Pen de marque Speedlink est ...   201115110   938777978   \n",
       "\n",
       "                                                text  \n",
       "0  Olivia: Personalisiertes Notizbuch / 150 Seite...  \n",
       "1  Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...  \n",
       "2  Grand Stylet Ergonomique Bleu Gamepad Nintendo...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.fillna('',inplace=True)\n",
    "X['text'] = X.apply(lambda line: line['designation'] + line['description'],axis=1)\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separer les données en train & text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60735</th>\n",
       "      <td>Carte Postale Typo Aimer - Kiub</td>\n",
       "      <td>Carte postale tendance de la collection Typo d...</td>\n",
       "      <td>2825941333</td>\n",
       "      <td>1208783386</td>\n",
       "      <td>Carte Postale Typo Aimer - KiubCarte postale t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9118</th>\n",
       "      <td>Garçon - Le Jeu De Plateau !</td>\n",
       "      <td>A propos : Il s¿agit d¿un jeu de cartes dans l...</td>\n",
       "      <td>89102802</td>\n",
       "      <td>856119038</td>\n",
       "      <td>Garçon - Le Jeu De Plateau !A propos : Il s¿ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55855</th>\n",
       "      <td>Royaume Des Animaux Ab À Asc</td>\n",
       "      <td></td>\n",
       "      <td>197015072</td>\n",
       "      <td>936925976</td>\n",
       "      <td>Royaume Des Animaux Ab À Asc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           designation  \\\n",
       "60735  Carte Postale Typo Aimer - Kiub   \n",
       "9118      Garçon - Le Jeu De Plateau !   \n",
       "55855     Royaume Des Animaux Ab À Asc   \n",
       "\n",
       "                                             description   productid  \\\n",
       "60735  Carte postale tendance de la collection Typo d...  2825941333   \n",
       "9118   A propos : Il s¿agit d¿un jeu de cartes dans l...    89102802   \n",
       "55855                                                      197015072   \n",
       "\n",
       "          imageid                                               text  \n",
       "60735  1208783386  Carte Postale Typo Aimer - KiubCarte postale t...  \n",
       "9118    856119038  Garçon - Le Jeu De Plateau !A propos : Il s¿ag...  \n",
       "55855   936925976                       Royaume Des Animaux Ab À Asc  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importer la classe train_test \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Séparer le jeu de données en données d'entraînement et données test \n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(X,y, test_size=0.2,random_state=42)\n",
    "\n",
    "X_train_text.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokeniser: texte -> sequence entier (index dans un dictionaire):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Définition du tokenizer\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=50000)\n",
    "# Mettre à jour le dictionnaire du tokenizer\n",
    "tokenizer.fit_on_texts(X_train_text.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Transformer chaque review X_text_train en une séquence d'entiers à l'aide de la méthode texts_to_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train_text.text)\n",
    "X_test = tokenizer.texts_to_sequences(X_test_text.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stocker le dictionnaire de correspondance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des dictionnaires\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = tokenizer.index_word\n",
    "vocab_size = tokenizer.num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Transfomer la liste de sequence X_train en tableau numpy à l'aide de la fonction pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 500\n",
    "X_train_txt = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen, padding='post')\n",
    "X_test_txt = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  198, 10698, 40528, ...,     0,     0,     0],\n",
       "       [ 2492,    10,    70, ...,     0,     0,     0],\n",
       "       [ 3611,    17,   461, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [ 1802,  7702,  1366, ...,     0,     0,     0],\n",
       "       [  268,  3140,  6942, ...,     0,     0,     0],\n",
       "       [15137,  5659,   219, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open('classes.json') as f:\n",
    "    categories = json.load(f)\n",
    "\n",
    "i = 0\n",
    "y_dict = {}\n",
    "for category in categories:\n",
    "    y_train = y_train.replace(category,categories[category])\n",
    "    y_test = y_test.replace(category,categories[category])\n",
    "    y_dict[i] = category\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10': 0,\n",
       " '1140': 1,\n",
       " '1160': 2,\n",
       " '1180': 3,\n",
       " '1280': 4,\n",
       " '1281': 5,\n",
       " '1300': 6,\n",
       " '1301': 7,\n",
       " '1302': 8,\n",
       " '1320': 9,\n",
       " '1560': 10,\n",
       " '1920': 11,\n",
       " '1940': 12,\n",
       " '2060': 13,\n",
       " '2220': 14,\n",
       " '2280': 15,\n",
       " '2403': 16,\n",
       " '2462': 17,\n",
       " '2522': 18,\n",
       " '2582': 19,\n",
       " '2583': 20,\n",
       " '2585': 21,\n",
       " '2705': 22,\n",
       " '2905': 23,\n",
       " '40': 24,\n",
       " '50': 25,\n",
       " '60': 26}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60735     9\n",
       "9118      5\n",
       "55855    16\n",
       "42138     8\n",
       "10948     0\n",
       "10697     9\n",
       "64862    22\n",
       "19280     6\n",
       "61325    10\n",
       "47489     4\n",
       "Name: prdtypecode, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de notre jeu de données\n",
    "text_train_set = tf.data.Dataset.from_tensor_slices((X_train_txt, y_train.values))\n",
    "\n",
    "text_test_set = tf.data.Dataset.from_tensor_slices((X_test_txt, y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter la fonction load_image dans le pipeline des opérations. Séparer le résultat en lot de taille 32.\n",
    "text_train_set = text_train_set.map(lambda text, y: [text, y]).batch(32).repeat(-1)\n",
    "#dataset = dataset.map(lambda x, y: [load_image(x), y[:-1], y[1:]]).batch(16).repeat(-1)\n",
    "\n",
    "text_test_set = text_test_set.map(lambda text, y: [text, y]).batch(32).repeat(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modele pour classification de texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'RNN'\n",
    "version = 'v5'\n",
    "model_name = model_type + '_' +  version\n",
    "model_path = 'models_output\\\\' + model_type + '\\\\' + version + '\\\\'\n",
    "model_path_rnn_v5 = model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, GlobalAveragePooling1D, RNN, GRU, Dense,Dropout\n",
    "\n",
    "embedding_dim = 256\n",
    "voc_size_inp = len(tokenizer.word_counts)+1\n",
    "\n",
    "text_inputs = Input(shape=(maxlen,), dtype='int32',name='input_' + model_name)\n",
    "x = Embedding(voc_size_inp, embedding_dim, name= 'embed_' + model_name)(text_inputs)\n",
    "#x = GRU(128, return_sequences=True, name='gru_' + model_name)(x)\n",
    "x = GRU(128,return_sequences=True, name = 'GRU_' + model_name)(x)\n",
    "#x = Dense(1024, activation='relu', name='dense_1_' + model_name)(x)\n",
    "x = Dropout(0.3, name='dropout_1' + model_name)(x)\n",
    "x = GlobalAveragePooling1D(name='batchnorm' + model_name)(x)\n",
    "RNN_v5 = Dense(256, activation='relu', name='dense_2_' + model_name)(x)\n",
    "#x = Dropout(0.3, name='dropout_2_' + model_name)(x)\n",
    "#text_output = Dense(27, activation='softmax', name='output_' + model_name)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT TRAITÉ (RNN_v6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Données textuelles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = pd.read_csv(r'C:\\Users\\Edgar\\Documents\\Rakuten\\X_train_update.csv',index_col =0)\n",
    "X_treat = pd.read_csv(r'C:\\Users\\Edgar\\Documents\\Rakuten\\X_train\\X_train_lemma-FR_stop_words-FR_no_num-FR_remove_accents-FR_no_special-FR_lemma-EN_stop_words-EN_stop_words-DE_lemma-DE_steem-FR_steem-EN_steem-DE.csv',index_col =0)\n",
    "y = pd.read_csv(r'C:\\Users\\Edgar\\Documents\\Rakuten\\Y_train_CVw08PX.csv',index_col=0).squeeze().map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre de mots par texte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_treat.rename(columns={'0':'text'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separer les données en train & text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60735</th>\n",
       "      <td>Carte Postale Typo Aimer - Kiub</td>\n",
       "      <td>Carte postale tendance de la collection Typo d...</td>\n",
       "      <td>2825941333</td>\n",
       "      <td>1208783386</td>\n",
       "      <td>Carte Postale Typo Aimer - KiubCarte postale t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9118</th>\n",
       "      <td>Garçon - Le Jeu De Plateau !</td>\n",
       "      <td>A propos : Il s¿agit d¿un jeu de cartes dans l...</td>\n",
       "      <td>89102802</td>\n",
       "      <td>856119038</td>\n",
       "      <td>Garçon - Le Jeu De Plateau !A propos : Il s¿ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55855</th>\n",
       "      <td>Royaume Des Animaux Ab À Asc</td>\n",
       "      <td></td>\n",
       "      <td>197015072</td>\n",
       "      <td>936925976</td>\n",
       "      <td>Royaume Des Animaux Ab À Asc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           designation  \\\n",
       "60735  Carte Postale Typo Aimer - Kiub   \n",
       "9118      Garçon - Le Jeu De Plateau !   \n",
       "55855     Royaume Des Animaux Ab À Asc   \n",
       "\n",
       "                                             description   productid  \\\n",
       "60735  Carte postale tendance de la collection Typo d...  2825941333   \n",
       "9118   A propos : Il s¿agit d¿un jeu de cartes dans l...    89102802   \n",
       "55855                                                      197015072   \n",
       "\n",
       "          imageid                                               text  \n",
       "60735  1208783386  Carte Postale Typo Aimer - KiubCarte postale t...  \n",
       "9118    856119038  Garçon - Le Jeu De Plateau !A propos : Il s¿ag...  \n",
       "55855   936925976                       Royaume Des Animaux Ab À Asc  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importer la classe train_test \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Séparer le jeu de données en données d'entraînement et données test \n",
    "X_train_text_treated, X_test_text_treated, y_train, y_test = train_test_split(X_treat,y, test_size=0.2,random_state=42)\n",
    "\n",
    "X_train_text.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokeniser: texte -> sequence entier (index dans un dictionaire):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Définition du tokenizer\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=50000)\n",
    "# Mettre à jour le dictionnaire du tokenizer\n",
    "tokenizer.fit_on_texts(X_train_text_treated.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Transformer chaque review X_text_train en une séquence d'entiers à l'aide de la méthode texts_to_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_treated = tokenizer.texts_to_sequences(X_train_text_treated.text)\n",
    "X_test_treated = tokenizer.texts_to_sequences(X_test_text_treated.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stocker le dictionnaire de correspondance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des dictionnaires\n",
    "word2idx_treated = tokenizer.word_index\n",
    "idx2word_treated = tokenizer.index_word\n",
    "vocab_size_treated = tokenizer.num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Transfomer la liste de sequence X_train en tableau numpy à l'aide de la fonction pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 500\n",
    "X_train_txt_treated = tf.keras.preprocessing.sequence.pad_sequences(X_train_treated, maxlen=maxlen, padding='post')\n",
    "X_test_txt_treated = tf.keras.preprocessing.sequence.pad_sequences(X_test_treated, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  112,  2600, 20822, ...,     0,     0,     0],\n",
       "       [  679,    31,   754, ...,     0,     0,     0],\n",
       "       [ 2721,   195,   410, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [ 1335,  4387,    63, ...,     0,     0,     0],\n",
       "       [  208,   736,  4951, ...,     0,     0,     0],\n",
       "       [ 2665,  1985,   143, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_txt_treated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open('classes.json') as f:\n",
    "    categories = json.load(f)\n",
    "\n",
    "i = 0\n",
    "y_dict = {}\n",
    "for category in categories:\n",
    "    y_train = y_train.replace(category,categories[category])\n",
    "    y_test = y_test.replace(category,categories[category])\n",
    "    y_dict[i] = category\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60735     9\n",
       "9118      5\n",
       "55855    16\n",
       "42138     8\n",
       "10948     0\n",
       "         ..\n",
       "6265     12\n",
       "54886    13\n",
       "76820    11\n",
       "860       8\n",
       "15795     0\n",
       "Name: prdtypecode, Length: 67932, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de notre jeu de données\n",
    "treated_text_train_set = tf.data.Dataset.from_tensor_slices((X_train_txt_treated, y_train.values))\n",
    "\n",
    "treated_text_test_set = tf.data.Dataset.from_tensor_slices((X_test_txt_treated, y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter la fonction load_image dans le pipeline des opérations. Séparer le résultat en lot de taille 32.\n",
    "treated_text_train_set = treated_text_train_set.map(lambda text, y: [text, y]).batch(32).repeat(-1)\n",
    "#dataset = dataset.map(lambda x, y: [load_image(x), y[:-1], y[1:]]).batch(16).repeat(-1)\n",
    "\n",
    "treated_text_test_set = treated_text_test_set.map(lambda text, y: [text, y]).batch(32).repeat(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modele pour classification de texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'RNN'\n",
    "version = 'v6'\n",
    "model_name = model_type + '_' +  version\n",
    "model_path = 'models_output\\\\' + model_type + '\\\\' + version + '\\\\'\n",
    "model_path_rnn_v6 = model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, GlobalAveragePooling1D, RNN, GRU, Dense,Dropout\n",
    "\n",
    "embedding_dim = 256\n",
    "voc_size_inp = len(tokenizer.word_counts)+1\n",
    "\n",
    "treated_text_inputs = Input(shape=(maxlen,), dtype='int32',name='input_' + model_name)\n",
    "x = Embedding(voc_size_inp, embedding_dim, name= 'embed_' + model_name)(treated_text_inputs)\n",
    "#x = GRU(128, return_sequences=True, name='gru_' + model_name)(x)\n",
    "x = GRU(128,return_sequences=True, name = 'GRU_' + model_name)(x)\n",
    "#x = Dense(1024, activation='relu', name='dense_1_' + model_name)(x)\n",
    "x = Dropout(0.3, name='dropout_1' + model_name)(x)\n",
    "x = GlobalAveragePooling1D(name='batchnorm' + model_name)(x)\n",
    "RNN_v6 = Dense(256, activation='relu', name='dense_2_' + model_name)(x)\n",
    "#x = Dropout(0.3, name='dropout_2_' + model_name)(x)\n",
    "#text_output = Dense(27, activation='softmax', name='output_' + model_name)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMAGES (EffNetB1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuperer les données images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_1263597046_product_3804725264.jpg\n",
      "C:\\Users\\Edgar\\Documents\\Rakuten\\images\\image_train\\image_1263597046_product_3804725264.jpg\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv('X_train_update.csv',index_col=0)\n",
    "y = pd.read_csv('Y_train_CVw08PX.csv',index_col=0).squeeze().map(str)\n",
    "\n",
    "#Create a column with the name of the picture\n",
    "X['image_name'] = 'image_' + X['imageid'].map(str) + '_product_' + X['productid'].map(str) + '.jpg'\n",
    "X['image_path'] = path + r'\\image_' + X['imageid'].map(str) + '_product_' + X['productid'].map(str) + '.jpg'\n",
    "print(X['image_name'].loc[0])\n",
    "print(X['image_path'].loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatener X_train et les labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>image_name</th>\n",
       "      <th>image_path</th>\n",
       "      <th>prdtypecode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "      <td>image_1263597046_product_3804725264.jpg</td>\n",
       "      <td>C:\\Users\\Edgar\\Documents\\Rakuten\\images\\image_...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>436067568</td>\n",
       "      <td>1008141237</td>\n",
       "      <td>image_1008141237_product_436067568.jpg</td>\n",
       "      <td>C:\\Users\\Edgar\\Documents\\Rakuten\\images\\image_...</td>\n",
       "      <td>2280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n",
       "      <td>201115110</td>\n",
       "      <td>938777978</td>\n",
       "      <td>image_938777978_product_201115110.jpg</td>\n",
       "      <td>C:\\Users\\Edgar\\Documents\\Rakuten\\images\\image_...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         designation  \\\n",
       "0  Olivia: Personalisiertes Notizbuch / 150 Seite...   \n",
       "1  Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...   \n",
       "2  Grand Stylet Ergonomique Bleu Gamepad Nintendo...   \n",
       "\n",
       "                                         description   productid     imageid  \\\n",
       "0                                                NaN  3804725264  1263597046   \n",
       "1                                                NaN   436067568  1008141237   \n",
       "2  PILOT STYLE Touch Pen de marque Speedlink est ...   201115110   938777978   \n",
       "\n",
       "                                image_name  \\\n",
       "0  image_1263597046_product_3804725264.jpg   \n",
       "1   image_1008141237_product_436067568.jpg   \n",
       "2    image_938777978_product_201115110.jpg   \n",
       "\n",
       "                                          image_path prdtypecode  \n",
       "0  C:\\Users\\Edgar\\Documents\\Rakuten\\images\\image_...          10  \n",
       "1  C:\\Users\\Edgar\\Documents\\Rakuten\\images\\image_...        2280  \n",
       "2  C:\\Users\\Edgar\\Documents\\Rakuten\\images\\image_...          50  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.concat([X,y],axis=1)\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois nous avons un dataset de la taille desirée on peut le séparer en train et test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_img, X_test_img, y_train, y_test = train_test_split(X[['image_name','prdtypecode']], X.prdtypecode ,test_size=0.2, random_state=42)\n",
    "#X_train_path, X_test_path, y_train, y_test = train_test_split(X.image_path, X.label, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>prdtypecode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60735</th>\n",
       "      <td>image_1208783386_product_2825941333.jpg</td>\n",
       "      <td>1320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9118</th>\n",
       "      <td>image_856119038_product_89102802.jpg</td>\n",
       "      <td>1281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55855</th>\n",
       "      <td>image_936925976_product_197015072.jpg</td>\n",
       "      <td>2403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    image_name prdtypecode\n",
       "60735  image_1208783386_product_2825941333.jpg        1320\n",
       "9118      image_856119038_product_89102802.jpg        1281\n",
       "55855    image_936925976_product_197015072.jpg        2403"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_img.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation des données:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 67932 validated image filenames belonging to 27 classes.\n",
      "Found 16984 validated image filenames belonging to 27 classes.\n"
     ]
    }
   ],
   "source": [
    "#APPLY SOME TRANSFORMATIONS TO DATA\n",
    "#from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "batch = 32\n",
    "\n",
    "\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(#rescale = 1./255,\n",
    "                                preprocessing_function = preprocess_input,\n",
    "                                   #shear_range = 0.5,\n",
    "                                   #zoom_range = 0.1,\n",
    "                                   #rotation_range=10,\n",
    "                                   #width_shift_range=0.1,\n",
    "                                   #height_shift_range=0.1,\n",
    "                                   #horizontal_flip=True,\n",
    "                                   # brightness_range = [0.9,1.1],\n",
    "                                   #fill_mode='nearest'\n",
    "                                )\n",
    "\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(#rescale = 1./255,\n",
    "                                                              preprocessing_function = preprocess_input\n",
    "                                                              )\n",
    "\n",
    "image_train_set = train_datagen.flow_from_dataframe(dataframe=X_train_img,\n",
    "                                              directory=path,\n",
    "                                              x_col = \"image_name\",\n",
    "                                              y_col = \"prdtypecode\",\n",
    "                                              class_mode =\"sparse\",\n",
    "                                              target_size = (224, 224),\n",
    "                                              batch_size = batch,\n",
    "                                                   shuffle=False)\n",
    "\n",
    "image_test_set = test_datagen.flow_from_dataframe(dataframe=X_test_img,\n",
    "                                              directory=path,\n",
    "                                              x_col = \"image_name\",\n",
    "                                              y_col = \"prdtypecode\",\n",
    "                                            class_mode =\"sparse\",\n",
    "                                              target_size = (224, 224),\n",
    "                                              batch_size = batch,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10': 0,\n",
       " '1140': 1,\n",
       " '1160': 2,\n",
       " '1180': 3,\n",
       " '1280': 4,\n",
       " '1281': 5,\n",
       " '1300': 6,\n",
       " '1301': 7,\n",
       " '1302': 8,\n",
       " '1320': 9,\n",
       " '1560': 10,\n",
       " '1920': 11,\n",
       " '1940': 12,\n",
       " '2060': 13,\n",
       " '2220': 14,\n",
       " '2280': 15,\n",
       " '2403': 16,\n",
       " '2462': 17,\n",
       " '2522': 18,\n",
       " '2582': 19,\n",
       " '2583': 20,\n",
       " '2585': 21,\n",
       " '2705': 22,\n",
       " '2905': 23,\n",
       " '40': 24,\n",
       " '50': 25,\n",
       " '60': 26}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_test_set.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modele pour classification d'images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'Functional'\n",
    "existing_model = 'EffNetB1'\n",
    "version = 'v2'\n",
    "filename = existing_model + '_' +  version\n",
    "model_path = 'models_output\\\\' + existing_model + '\\\\' + version + '\\\\'\n",
    "model_path_effnetb1_v2 = model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Input, GlobalAveragePooling2D\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "base_model = tf.keras.applications.EfficientNetB1(weights='imagenet',input_shape=(224, 224, 3),include_top=False)\n",
    "\n",
    "image_input = Input(shape=(224, 224, 3), name= 'input_' + filename)\n",
    "x = base_model(image_input)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu',name= 'dense_' + filename)(x)\n",
    "#x = BatchNormalization(trainable = True,axis=1,name= 'batchnorm' + model_name)(x)\n",
    "x = Dropout(0.5,name= 'dropout_' + filename)(x)\n",
    "x = Dense(512, activation='relu',name= 'dense_2_' + filename)(x)\n",
    "EffNetB1 = Dropout(0.2,name= 'dropout_2_' + filename)(x)\n",
    "#x = Flatten()(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generateurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation des données:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition d'un générateur python\n",
    "def generator(image_set,text_set,treated_text_set):\n",
    "    iter_image = iter(image_set)\n",
    "    iter_text = iter(text_set)\n",
    "    iter_text_treated = iter(treated_text_set)\n",
    "    while True:\n",
    "        X_im, y = next(iter_image)\n",
    "        X_text, y_text = next(iter_text) \n",
    "        X_text_treated, y_text = next(iter_text_treated) \n",
    "        \n",
    "        #print('y_text', y_text)\n",
    "        #print('')\n",
    "        #print('y_image', y)\n",
    "        #print('')\n",
    "        \n",
    "        \n",
    "        #print('')\n",
    "        #print('X_im:',X_im.shape,'X_text:',X_text.shape,'y:',y.shape)\n",
    "        #print(y)\n",
    "        yield [X_im, X_text, X_text_treated], y_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du générateur final.\n",
    "gen_train = generator(image_train_set,text_train_set,treated_text_train_set)\n",
    "\n",
    "gen_test = generator(image_test_set,text_test_set,treated_text_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCATENATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_EffNetB1_v2 (InputLayer)  [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "efficientnetb1 (Functional)     (None, 7, 7, 1280)   6575239     input_EffNetB1_v2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "input_RNN_v5 (InputLayer)       [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_RNN_v6 (InputLayer)       [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 1280)         0           efficientnetb1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embed_RNN_v5 (Embedding)        (None, 500, 256)     47877376    input_RNN_v5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embed_RNN_v6 (Embedding)        (None, 500, 256)     20059648    input_RNN_v6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_EffNetB1_v2 (Dense)       (None, 1024)         1311744     global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "GRU_RNN_v5 (GRU)                (None, 500, 128)     148224      embed_RNN_v5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "GRU_RNN_v6 (GRU)                (None, 500, 128)     148224      embed_RNN_v6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_EffNetB1_v2 (Dropout)   (None, 1024)         0           dense_EffNetB1_v2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1RNN_v5 (Dropout)       (None, 500, 128)     0           GRU_RNN_v5[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1RNN_v6 (Dropout)       (None, 500, 128)     0           GRU_RNN_v6[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_2_EffNetB1_v2 (Dense)     (None, 512)          524800      dropout_EffNetB1_v2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batchnormRNN_v5 (GlobalAverageP (None, 128)          0           dropout_1RNN_v5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnormRNN_v6 (GlobalAverageP (None, 128)          0           dropout_1RNN_v6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2_EffNetB1_v2 (Dropout) (None, 512)          0           dense_2_EffNetB1_v2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2_RNN_v5 (Dense)          (None, 256)          33024       batchnormRNN_v5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2_RNN_v6 (Dense)          (None, 256)          33024       batchnormRNN_v6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1024)         0           dropout_2_EffNetB1_v2[0][0]      \n",
      "                                                                 dense_2_RNN_v5[0][0]             \n",
      "                                                                 dense_2_RNN_v6[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_concat (Dense)            (None, 1024)         1049600     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_concat (Dropout)        (None, 1024)         0           dense_concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_concat_final (Dense)      (None, 27)           27675       dropout_concat[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 77,788,578\n",
      "Trainable params: 77,726,523\n",
      "Non-trainable params: 62,055\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#https://towardsdatascience.com/deep-multi-input-models-transfer-learning-for-image-and-word-tag-recognition-7ae0462253dc\n",
    "\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "x = concatenate([EffNetB1, RNN_v5, RNN_v6], axis=-1)\n",
    "x = Dense(1024, activation='relu',name='dense_' + 'concat')(x)\n",
    "x = Dropout(0.3,name= 'dropout_' + 'concat')(x)\n",
    "output = Dense(27, activation='softmax',name='dense_' + 'concat_final')(x)\n",
    "\n",
    "model = Model([image_input, text_inputs, treated_text_inputs], output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charger les poids des modèles déjà entrainés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_old_models_weights(concat_model,model_name,model_path):\n",
    "    \"\"\"\n",
    "    Copie des poids des layers des sous modèles de bases vers leurs clones\n",
    "    (ayant le même nom) du modèle courant\n",
    "    \"\"\"\n",
    "    \n",
    "    for model_name, path in zip(model_name,model_path):\n",
    "        old_model = tf.keras.models.load_model(path + model_name + '.hdf5')\n",
    "        old_model.load_weights(path + model_name + '.hdf5')\n",
    "\n",
    "        for old_layer in old_model.layers:\n",
    "            weights = old_layer.get_weights()\n",
    "            if len(weights) > 0:\n",
    "                for new_layer in concat_model.layers:\n",
    "                    if new_layer.name == old_layer.name:\n",
    "                        print(f\"      - {new_layer.name}\")\n",
    "                        new_layer.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      - embed_RNN_v5\n",
      "      - GRU_RNN_v5\n",
      "      - dense_2_RNN_v5\n",
      "      - embed_RNN_v6\n",
      "      - GRU_RNN_v6\n",
      "      - dense_2_RNN_v6\n",
      "      - efficientnetb1\n",
      "      - dense_EffNetB1_v2\n",
      "      - dense_2_EffNetB1_v2\n"
     ]
    }
   ],
   "source": [
    "models_name = ['RNN_v5','RNN_v6','EffNetB1_v2']\n",
    "models_path = [model_path_rnn_v5,model_path_rnn_v6,model_path_effnetb1_v2]\n",
    "\n",
    "copy_old_models_weights(model,models_name,models_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixer les layers déjà entrainés et débloquer que celles du concat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unfreeze the following layers:\n",
      "dense_concat\n",
      "dropout_concat\n",
      "dense_concat_final\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_EffNetB1_v2 (InputLayer)  [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "efficientnetb1 (Functional)     (None, 7, 7, 1280)   6575239     input_EffNetB1_v2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "input_RNN_v5 (InputLayer)       [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_RNN_v6 (InputLayer)       [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 1280)         0           efficientnetb1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embed_RNN_v5 (Embedding)        (None, 500, 256)     47877376    input_RNN_v5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embed_RNN_v6 (Embedding)        (None, 500, 256)     20059648    input_RNN_v6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_EffNetB1_v2 (Dense)       (None, 1024)         1311744     global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "GRU_RNN_v5 (GRU)                (None, 500, 128)     148224      embed_RNN_v5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "GRU_RNN_v6 (GRU)                (None, 500, 128)     148224      embed_RNN_v6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_EffNetB1_v2 (Dropout)   (None, 1024)         0           dense_EffNetB1_v2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1RNN_v5 (Dropout)       (None, 500, 128)     0           GRU_RNN_v5[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1RNN_v6 (Dropout)       (None, 500, 128)     0           GRU_RNN_v6[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_2_EffNetB1_v2 (Dense)     (None, 512)          524800      dropout_EffNetB1_v2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batchnormRNN_v5 (GlobalAverageP (None, 128)          0           dropout_1RNN_v5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnormRNN_v6 (GlobalAverageP (None, 128)          0           dropout_1RNN_v6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2_EffNetB1_v2 (Dropout) (None, 512)          0           dense_2_EffNetB1_v2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2_RNN_v5 (Dense)          (None, 256)          33024       batchnormRNN_v5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2_RNN_v6 (Dense)          (None, 256)          33024       batchnormRNN_v6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1024)         0           dropout_2_EffNetB1_v2[0][0]      \n",
      "                                                                 dense_2_RNN_v5[0][0]             \n",
      "                                                                 dense_2_RNN_v6[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_concat (Dense)            (None, 1024)         1049600     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_concat (Dropout)        (None, 1024)         0           dense_concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_concat_final (Dense)      (None, 27)           27675       dropout_concat[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 77,788,578\n",
      "Trainable params: 1,077,275\n",
      "Non-trainable params: 76,711,303\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers: \n",
    "    layer.trainable = False\n",
    "\n",
    "print('unfreeze the following layers:')\n",
    "for layer in model.layers:\n",
    "    if '_concat' in layer.name:\n",
    "        print(layer.name)\n",
    "        layer.trainable = True\n",
    "        \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model compile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_model = 'Concat'\n",
    "version = 'v1'\n",
    "model_path = 'models_output\\\\' + existing_model + '\\\\' + version + '\\\\'\n",
    "filename = 'RNN_V5' + '_' + 'RNN_V6' + '_' + 'EffNetb1_v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                                         patience=3,\n",
    "                                         mode='max',\n",
    "                                         restore_best_weights=True)\n",
    "\n",
    "\n",
    "checkpoint = callbacks.ModelCheckpoint(filepath= model_path + filename + '.hdf5', \n",
    "                                       monitor='val_accuracy',\n",
    "                                       save_best_only=True,\n",
    "                                       save_weights_only=False,\n",
    "                                       mode='max',\n",
    "                                       save_freq='epoch')\n",
    "\n",
    "red_on_plateau = callbacks.ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                             patience=2, \n",
    "                                             factor=0.1,\n",
    "                                             verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrainement du modèle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2122/2122 [==============================] - 5050s 2s/step - loss: 0.0364 - accuracy: 0.9919 - val_loss: 2.3788 - val_accuracy: 0.8285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Edgar\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "2122/2122 [==============================] - 4865s 2s/step - loss: 0.0293 - accuracy: 0.9949 - val_loss: 2.7721 - val_accuracy: 0.8325\n",
      "Epoch 3/10\n",
      "2122/2122 [==============================] - 4862s 2s/step - loss: 0.0254 - accuracy: 0.9958 - val_loss: 3.5530 - val_accuracy: 0.8322\n",
      "Epoch 4/10\n",
      "2122/2122 [==============================] - 4866s 2s/step - loss: 0.0219 - accuracy: 0.9964 - val_loss: 3.6418 - val_accuracy: 0.8324\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 5/10\n",
      "2122/2122 [==============================] - 4874s 2s/step - loss: 0.0100 - accuracy: 0.9980 - val_loss: 3.4681 - val_accuracy: 0.8373\n",
      "Epoch 6/10\n",
      "2122/2122 [==============================] - 4886s 2s/step - loss: 0.0079 - accuracy: 0.9982 - val_loss: 3.4094 - val_accuracy: 0.8373\n",
      "Epoch 7/10\n",
      "2122/2122 [==============================] - 4865s 2s/step - loss: 0.0064 - accuracy: 0.9984 - val_loss: 3.3906 - val_accuracy: 0.8375\n",
      "Epoch 8/10\n",
      "2122/2122 [==============================] - 4867s 2s/step - loss: 0.0049 - accuracy: 0.9988 - val_loss: 3.3401 - val_accuracy: 0.8377\n",
      "Epoch 9/10\n",
      "2122/2122 [==============================] - 4876s 2s/step - loss: 0.0046 - accuracy: 0.9987 - val_loss: 3.2828 - val_accuracy: 0.8386\n",
      "Epoch 10/10\n",
      "2122/2122 [==============================] - 4878s 2s/step - loss: 0.0044 - accuracy: 0.9988 - val_loss: 3.3095 - val_accuracy: 0.8390\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24a208653a0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_steps = int(len(y_train.values)/32)\n",
    "validation_steps = int(len(y_test.values)/32)\n",
    "\n",
    "model.fit(gen_train,\n",
    "          steps_per_epoch = train_steps,\n",
    "          validation_data = gen_test,\n",
    "          validation_steps = validation_steps,\n",
    "          epochs=10,\n",
    "          workers=1,\n",
    "         callbacks=[early_stopping, checkpoint,red_on_plateau])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
