{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf4040ff",
   "metadata": {},
   "source": [
    "Concatenation des vecteurs issues de TfidfVectorizer+SelectFromModel(LinearSVC) avec ceux issus d'un embedding (multilingual).  \n",
    "\n",
    "Entrainement avec des modèles machine learning sur **40000** échantillons:\n",
    "  * LinearSVC(l2) est moins bon (0.71) qu'avec TfidfVectorizer seul (0.82) et semble diverger (?)\n",
    "  * LightGBM a des résultats corrects (0.8176) dans un temps raisonnable (687 sec)\n",
    "  * Le petit réseau de neurones est à 0.79\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "agreed-tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_text\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "OUTDIR = \"out\"\n",
    "if not os.path.isdir(OUTDIR):\n",
    "    os.mkdir(OUTDIR)\n",
    "\n",
    "\n",
    "def f1_score_w(y_true, y_pred, **kwargs):\n",
    "    \"\"\"\n",
    "    Score utilisé par Rakuten\n",
    "    \"\"\"\n",
    "    return round(f1_score(y_true, y_pred, average='weighted'),4)\n",
    "    \n",
    "# A virer\n",
    "\n",
    "spacynlp = None\n",
    "spacyre = re.compile(r\"(<.*?>|&#\\d+;|\\'|\\:|\\.|\\-|\\+)\")\n",
    "french_stop_words = set(stopwords.words('french'))\n",
    "def tokenize_spacy(sentence):\n",
    "    \"\"\"\n",
    "    Tokenizer basé sur spacy\n",
    "    \"\"\"\n",
    "    global spacynlp\n",
    "    if spacynlp is None:\n",
    "        spacynlp = spacy.load(\"fr_core_news_sm\")\n",
    "        spacynlp.disable_pipes ('tagger', 'parser', 'ner')\n",
    "    s = sentence\n",
    "    t = [x.lemma_ for x in spacynlp(s)]\n",
    "    #t = [x.text for x in spacynlp(s) if not x.text in french_stop_words]\n",
    "    return t\n",
    "\n",
    "def get_text(df):\n",
    "    lst = []\n",
    "    for desc, desi in zip(df.designation, df.description):\n",
    "        desistr = desi if type(desi) == str else ''\n",
    "        descstr = desc if type(desc) == str else ''\n",
    "        sep = ' ' if type(desc) == str and type(desi) == str else ''\n",
    "        s = desistr + sep + descstr\n",
    "        s = s.lower() + ' ' + re.sub(r\"([^A-Z0-9°\\+\\*\\=]+)\",\"\",s).lower()\n",
    "        lst.append(s)\n",
    "    return lst\n",
    "\n",
    "def get_clean_df(test_data=False):\n",
    "    \"\"\"\n",
    "    Lecture d'un fichier de données, avec petit nettoyage et sauvegarde\n",
    "    Utilisation de la sauvegarde si elle existe\n",
    "\n",
    "    \"\"\"\n",
    "    if test_data == True:\n",
    "        xfile = \"X_test_update.csv\"\n",
    "        imdir = os.path.join(\"images\", \"image_test\")\n",
    "        savef = os.path.join(OUTDIR, \"clean_test_df.pkl\")\n",
    "    else:\n",
    "        xfile = \"X_train_update.csv\"\n",
    "        imdir = os.path.join(\"images\", \"image_train\")\n",
    "        savef = os.path.join(OUTDIR, \"clean_train_df.pkl\")\n",
    "    #\n",
    "    if os.path.isfile(savef):\n",
    "        print(f\"Lecture de {savef}\")\n",
    "        return pd.read_pickle(savef)\n",
    "    #\n",
    "    print(f\"Construction de {savef}\")\n",
    "    df = pd.read_csv(xfile).drop(\"Unnamed: 0\", axis=1)\n",
    "    df['imgpath'] = df.apply(lambda x: os.path.join(imdir,\n",
    "                                                   \"image_%d_product_%d.jpg\" % (x.imageid,\n",
    "                                                                                x.productid)),\n",
    "                                                   axis=1)\n",
    "    df.designation = df.designation.astype('string')\n",
    "    df.description = df.description.astype('string')\n",
    "    if not test_data:\n",
    "        y = pd.read_csv(\"Y_train_CVw08PX.csv\").drop(\"Unnamed: 0\", axis=1)\n",
    "        df['class'] = y.astype(str) # Needed by generator\n",
    "    df.to_pickle(savef)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ranking-negotiation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lecture de out\\clean_train_df.pkl\n"
     ]
    }
   ],
   "source": [
    "df = get_clean_df()[:40000]\n",
    "X_train, X_test, y_train, y_test = train_test_split(get_text(df), df['class'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=51)\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=52)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lasting-citation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer +  SelectFromModel(LinearSVC) ...\n",
      "Shape (X_train_tfidf) = (32000, 10927) (18 sec)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_i2l = list(df['class'].value_counts().index)\n",
    "y_l2i = { y_i2l[i]:i for i in range(len(y_i2l))}\n",
    "\n",
    "y_train_i = np.array(y_train.apply(lambda x: y_l2i[x]))\n",
    "y_test_i = np.array(y_test.apply(lambda x: y_l2i[x]))\n",
    "\n",
    "\n",
    "print(f\"TfidfVectorizer +  SelectFromModel(LinearSVC) ...\")\n",
    "start = time()\n",
    "tfidf = TfidfVectorizer(analyzer='word',\n",
    "                  #strip_accents='ascii',\n",
    "                  stop_words=french_stop_words,\n",
    "                  max_df=0.8,\n",
    "                  min_df=1,\n",
    "                  ngram_range=(1,1),\n",
    "                  use_idf=False,\n",
    "                  smooth_idf=False,\n",
    "                  sublinear_tf=False,\n",
    "                  binary=False,\n",
    "                  #max_features=10000,\n",
    "                  )\n",
    "selector = SelectFromModel(LinearSVC(penalty=\"l1\", dual=False,\n",
    "                                     tol=1e-4, max_iter=5000))\n",
    "#selector = SelectFromModel(LinearSVC(penalty=\"l2\", dual=True, C=0.8,\n",
    "#                                     tol=1e-5, max_iter=4000))\n",
    "X_train_tfidf = selector.fit_transform(tfidf.fit_transform(X_train), y_train).toarray()\n",
    "X_test_tfidf = selector.transform(tfidf.transform(X_test)).toarray()\n",
    "print(f\"Shape (X_train_tfidf) = {X_train_tfidf.shape} ({int(time()-start)} sec)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "polished-township",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding  ...\n",
      "Shape (X_train_embed) = (32000, 512) (4004 sec)\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "print(f\"Embedding  ...\")\n",
    "start = time()\n",
    "\n",
    "#embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")\n",
    "##embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")\n",
    "#embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-xling/en-fr/1\")\n",
    "embed = hub.load(\"tfhub/universal-sentence-encoder-multilingual-large-3\")\n",
    "\n",
    "\n",
    "X_train_embed = np.array([embed(x) for x in X_train]).reshape(len(X_train),-1)\n",
    "X_test_embed = np.array([embed(x) for x in X_test]).reshape(len(X_test),-1)\n",
    "print(f\"Shape (X_train_embed) = {X_train_embed.shape} ({int(time()-start)} sec)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc7a5c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenate + Standardize ...\n",
      "Shape (X_train_concat) = (32000, 11439) (19 sec)\n",
      "Sequential NN ...\n",
      "Epoch 1/15\n",
      "800/800 [==============================] - 10s 11ms/step - loss: 2.8281 - accuracy: 0.2824 - val_loss: 1.2874 - val_accuracy: 0.6853\n",
      "Epoch 2/15\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 1.1992 - accuracy: 0.7058 - val_loss: 0.9454 - val_accuracy: 0.7603\n",
      "Epoch 3/15\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.7374 - accuracy: 0.8203 - val_loss: 0.8127 - val_accuracy: 0.7872\n",
      "Epoch 4/15\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.4822 - accuracy: 0.8894 - val_loss: 0.7506 - val_accuracy: 0.8048\n",
      "Epoch 5/15\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.3163 - accuracy: 0.9291 - val_loss: 0.7436 - val_accuracy: 0.8114\n",
      "Epoch 6/15\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.2207 - accuracy: 0.9549 - val_loss: 0.7635 - val_accuracy: 0.8142\n",
      "Epoch 7/15\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.1593 - accuracy: 0.9696 - val_loss: 0.8073 - val_accuracy: 0.8163\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Score rnn = 0.7903\n",
      "Comparaisons de modèles machine learning...\n",
      "LightGBM w-f1-score = 0.8176 (687 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emarchan\\Anaconda3\\envs\\ScientestProjet\\lib\\site-packages\\sklearn\\svm\\_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC_custom w-f1-score = 0.7163 (31263 sec)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Concatenate + Standardize ...\")\n",
    "start = time()\n",
    "X_train_concat = np.concatenate([X_train_embed, X_train_tfidf], axis = 1)\n",
    "X_test_concat = np.concatenate([X_test_embed, X_test_tfidf], axis = 1)\n",
    "\n",
    "std = StandardScaler()\n",
    "X_train_concat = std.fit_transform(X_train_concat)\n",
    "X_test_concat = std.transform(X_test_concat)\n",
    "print(f\"Shape (X_train_concat) = {X_train_concat.shape} ({int(time()-start)} sec)\")\n",
    "\n",
    "print(f\"Sequential NN ...\")\n",
    "\n",
    "clf_rn = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(128, activation='relu', input_shape=X_train_concat.shape[1:]),\n",
    "                tf.keras.layers.Dropout(0.3),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dense(27, activation='softmax')\n",
    "])\n",
    "\n",
    "clf_rn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "               loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                  patience=2, verbose=1,\n",
    "                                                  restore_best_weights=True)\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=2)\n",
    "\n",
    "history = clf_rn.fit(x=X_train_concat, y=y_train_i, batch_size=32,\n",
    "                     validation_split=0.2,\n",
    "                     callbacks = [early_stopping, reduce_lr],\n",
    "                     epochs=15)\n",
    "y_test_pred_raw = clf_rn.predict(X_test_concat)\n",
    "y_pred_i = np.argmax(y_test_pred_raw, axis=1)\n",
    "print(f\"Score rnn = {f1_score_w(y_test_i, y_pred_i)}\")\n",
    "\n",
    "print(f\"Comparaisons de modèles machine learning...\")\n",
    "\n",
    "clfs = {#\"RandomForestClassifier\": RandomForestClassifier(),\n",
    "        #\"KNeighborsClassifier\": KNeighborsClassifier(),\n",
    "        #\"LinearSVC\": LinearSVC(),\n",
    "        \"LightGBM\" : LGBMClassifier(n_estimators=150, learning_rate=0.07,\n",
    "                     boosting_type='gbdt', class_weight='balanced'),\n",
    "        \"LinearSVC_custom\": LinearSVC(penalty=\"l2\", dual=True, C=0.8, tol=1e-5, max_iter=4000),\n",
    "       }\n",
    "for clfname in clfs:\n",
    "    clf = clfs[clfname]\n",
    "    t0 = time()\n",
    "    clf.fit(X_train_concat, y_train)\n",
    "    tfit = int(time() - t0)\n",
    "    y_pred = clf.predict(X_test_concat)\n",
    "    score = f1_score_w(y_test, y_pred)\n",
    "    print(f\"{clfname} w-f1-score = {score} ({tfit} sec)\")\n",
    "\n",
    "# 20000\n",
    "#   LightGBM w-f1-score = 0.7904 (507 sec)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77730beb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
