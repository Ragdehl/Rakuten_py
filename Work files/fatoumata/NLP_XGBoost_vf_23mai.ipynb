{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\barry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import unidecode\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\barry\\\\OneDrive - CSTBGroup\\\\image_ds\\\\images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('X_train_update.csv',index_col=0)\n",
    "y = pd.read_csv('Y_train_CVw08PX.csv',index_col=0)\n",
    "X = X.drop([\"productid\", \"imageid\", \"description\"], axis = 1)\n",
    "df= X.merge(y,  right_index = True, left_index = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/diveki/classification-with-nlp-xgboost-and-pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Suppression des duplicats car peut donner trop de poids à des mots\n",
    "##### Suppression des lignes avec valeurs manquantes car on ne peut les combler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>prdtypecode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...</td>\n",
       "      <td>2280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Peluche Donald - Europe - Disneyland 2000 (Mar...</td>\n",
       "      <td>1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La Guerre Des Tuques</td>\n",
       "      <td>2705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         designation  prdtypecode\n",
       "0  Olivia: Personalisiertes Notizbuch / 150 Seite...           10\n",
       "1  Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...         2280\n",
       "2  Grand Stylet Ergonomique Bleu Gamepad Nintendo...           50\n",
       "3  Peluche Donald - Europe - Disneyland 2000 (Mar...         1280\n",
       "4                               La Guerre Des Tuques         2705"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_single = df.drop_duplicates('designation')\n",
    "data_single = data_single.dropna(subset=['designation'])\n",
    "data_single.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Représentation du label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2583    9717\n",
       "1300    4988\n",
       "2522    4930\n",
       "1560    4922\n",
       "1280    4777\n",
       "2403    4763\n",
       "2280    4753\n",
       "2060    4540\n",
       "1160    3952\n",
       "1920    3836\n",
       "1320    3196\n",
       "10      3114\n",
       "2705    2753\n",
       "1140    2670\n",
       "2582    2497\n",
       "40      2478\n",
       "2585    2428\n",
       "1302    2302\n",
       "1281    1986\n",
       "50      1633\n",
       "2462    1406\n",
       "2905     872\n",
       "2220     817\n",
       "1940     802\n",
       "60       774\n",
       "1180     764\n",
       "1301     595\n",
       "Name: prdtypecode, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_single.iloc[:,-1].value_counts()\n",
    "# Il y a un certain déséquilibrage dans la représentation des labels, voir si on fera un RandomOverSample\n",
    "# ou UnderSample ou techniques similaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travail sur le texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout en miniscule : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data_single.iloc[:,:-1]:\n",
    "    data_single[col] = data_single[col].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppression des accents : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unidecode_text(text):\n",
    "    try:\n",
    "        text = unidecode.unidecode(text)\n",
    "    except:\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "for col in data_single.iloc[:,:-1]:\n",
    "    data_single[col] = data_single.apply(lambda row: unidecode_text(row[col]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mots vides : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_words = stopwords.words(\"english\")\n",
    "germ_words =stopwords.words(\"german\")\n",
    "french_words =stopwords.words(\"french\")\n",
    "\n",
    "stop_words1 = eng_words + germ_words + french_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Racinisation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "defTags = ['NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'JJS', 'JJR']#, 'RB', 'RBS', 'RBR', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "\n",
    "# functions to determine the type of a word\n",
    "def is_noun(tag):\n",
    "    return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "\n",
    "def is_verb(tag):\n",
    "    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "\n",
    "def is_adverb(tag):\n",
    "    return tag in ['RB', 'RBR', 'RBS']\n",
    "\n",
    "\n",
    "def is_adjective(tag):\n",
    "    return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "# transform tag forms\n",
    "def penn_to_wn(tag):\n",
    "    if is_adjective(tag):\n",
    "        return nltk.stem.wordnet.wordnet.ADJ\n",
    "    elif is_noun(tag):\n",
    "        return nltk.stem.wordnet.wordnet.NOUN\n",
    "    elif is_adverb(tag):\n",
    "        return nltk.stem.wordnet.wordnet.ADV\n",
    "    elif is_verb(tag):\n",
    "        return nltk.stem.wordnet.wordnet.VERB\n",
    "    return nltk.stem.wordnet.wordnet.NOUN\n",
    "\n",
    "\n",
    "# lemmatizer + tokenizer (+ stemming) class\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        # we define (but not use) a stemming method, uncomment the last line in __call__ to get stemming tooo\n",
    "        self.stemmer = nltk.stem.SnowballStemmer('english') \n",
    "    def __call__(self, doc):\n",
    "        # pattern for numbers | words of length=2 | punctuations | words of length=1\n",
    "        pattern = re.compile(r'[0-9]+|\\b[\\w]{2,2}\\b|[%.,_`!\"&?\\')({~@;:#}+-]+|\\b[\\w]{1,1}\\b')\n",
    "        # tokenize document\n",
    "        doc_tok = word_tokenize(doc)\n",
    "        #filter out patterns from words\n",
    "        doc_tok = [x for x in doc_tok if x not in stop_words1]\n",
    "        doc_tok = [pattern.sub('', x) for x in doc_tok]\n",
    "        # get rid of anything with length=1\n",
    "        doc_tok = [x for x in doc_tok if len(x) > 1]\n",
    "        # position tagging\n",
    "        doc_tagged = nltk.pos_tag(doc_tok)\n",
    "        # selecting nouns and adjectives\n",
    "        doc_tagged = [(t[0], t[1]) for t in doc_tagged if t[1] in defTags]\n",
    "        # preparing lemmatization\n",
    "        doc = [(t[0], penn_to_wn(t[1])) for t in doc_tagged]\n",
    "        # lemmatization\n",
    "        doc = [self.wnl.lemmatize(t[0], t[1]) for t in doc]\n",
    "        # uncomment if you want stemming as well\n",
    "        #doc = [self.stemmer.stem(x) for x in doc]\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf Vectorizer (mise en forme pour modélisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_tdidf = TfidfVectorizer(ngram_range=(1,1), analyzer='word', stop_words=stop_words1, \n",
    "                                               norm='l2', tokenizer=LemmaTokenizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modélisation XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = data_single.copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['designation'], df['prdtypecode'], \n",
    "                                                    test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_val, y_val = train_test_split(X_train, y_train, \n",
    "                                                    test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier(random_state=42, seed=2, colsample_bytree=0.6, subsample=0.7, learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline composante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations\n",
    "    on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None, *parg, **kwarg):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # returns the input as a string\n",
    "        return X[self.key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction pour l'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(preds, target, labels, sep='-', sep_len=40, fig_size=(10,8)):\n",
    "    print('Accuracy = %.3f' % metrics.accuracy_score(target, preds))\n",
    "    print(sep*sep_len)\n",
    "    print('Classification report:')\n",
    "    print(metrics.classification_report(target, preds))\n",
    "    print(sep*sep_len)\n",
    "    print('Confusion matrix')\n",
    "    cm=metrics.confusion_matrix(target, preds)\n",
    "    cm = cm / np.sum(cm, axis=1)[:,None]\n",
    "    sns.set(rc={'figure.figsize':fig_size})\n",
    "    sns.heatmap(cm, \n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels,\n",
    "           annot=True, cmap = 'YlGnBu')\n",
    "    plt.pause(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = Pipeline([\n",
    "                ('selector', TextSelector(key='designation')),\n",
    "                ('vectorizer', vec_tdidf)\n",
    "                ])\n",
    "\n",
    "feats = FeatureUnion([\n",
    "                      ('designation', text)\n",
    "                      ])\n",
    "\n",
    "pipe = Pipeline([('feats', feats),\n",
    "                 ('clf',clf)\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamétrisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liste paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary:logistic',\n",
       " 'use_label_encoder': True,\n",
       " 'base_score': None,\n",
       " 'booster': None,\n",
       " 'colsample_bylevel': None,\n",
       " 'colsample_bynode': None,\n",
       " 'colsample_bytree': 0.6,\n",
       " 'gamma': None,\n",
       " 'gpu_id': None,\n",
       " 'importance_type': 'gain',\n",
       " 'interaction_constraints': None,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_delta_step': None,\n",
       " 'max_depth': None,\n",
       " 'min_child_weight': None,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': None,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'num_parallel_tree': None,\n",
       " 'random_state': 42,\n",
       " 'reg_alpha': None,\n",
       " 'reg_lambda': None,\n",
       " 'scale_pos_weight': None,\n",
       " 'subsample': 0.7,\n",
       " 'tree_method': None,\n",
       " 'validate_parameters': None,\n",
       " 'verbosity': None,\n",
       " 'seed': 2}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps['clf'].get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liste hyperparamètres à tester dans la GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = [ 0.01, 0.1, 0.3]\n",
    "\n",
    "param_grid = {'clf__learning_rate': learning_rate,\n",
    "     'clf__n_estimators': [50,100,300], \n",
    "#    'clf__colsample_bytree': [0.6,0.8,1],\n",
    "#    'clf__subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search Paramétrisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=7)\n",
    "grid_search = GridSearchCV(estimator = pipe, param_grid = param_grid, \n",
    "                          cv = kfold, n_jobs = -1, return_train_score=True, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_input = data_single.copy()\n",
    "combined_features = [ 'designation']\n",
    "target = 'prdtypecode'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(kaggle_input[[ 'designation']], kaggle_input[target], \n",
    "                                                    test_size=0.33, random_state=42, stratify=kaggle_input[target])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recherche hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résultats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.cv_results_['mean_train_score'])\n",
    "grid_search.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prédiction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_test = grid_search.best_estimator_\n",
    "preds = clf_test.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stats(y_test, preds, clf_test.classes_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
