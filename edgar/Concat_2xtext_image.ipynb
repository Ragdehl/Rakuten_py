{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  DEEPLEARNING IMAGES & TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Edgar\\\\Documents\\\\Rakuten\\\\images\\\\image_train'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import os #Miscellaneous operating system interfaces\n",
    "#https://docs.python.org/3/library/os.html\n",
    "#get current working directory\n",
    "path = os.getcwd() + '\\\\images\\\\image_train'\n",
    "path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_old_models_weights(model,old_model_name,model_path):\n",
    "    \"\"\"\n",
    "    Copie des poids des layers des sous modèles de bases vers leurs clones\n",
    "    (ayant le même nom) du modèle courant\n",
    "    \"\"\"\n",
    "    old_model = tf.keras.models.load_model(model_path + old_model_name + '.hdf5')\n",
    "    old_model.load_weights(model_path + old_model_name + '.hdf5')\n",
    "    \n",
    "    \n",
    "    for old_layer in old_model.layers:\n",
    "        weights = old_layer.get_weights()\n",
    "        if len(weights) > 0:\n",
    "            for new_layer in model.layers:\n",
    "                if new_layer.name == old_layer.name:\n",
    "                    print(f\"      - {new_layer.name}\")\n",
    "                    newlayer.set_weights(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NON TRAITÉ (RNN_v4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Données textuelles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(r'C:\\Users\\Edgar\\Documents\\Rakuten\\X_train_update.csv',index_col =0)\n",
    "y = pd.read_csv(r'C:\\Users\\Edgar\\Documents\\Rakuten\\Y_train_CVw08PX.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "      <td></td>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...</td>\n",
       "      <td></td>\n",
       "      <td>436067568</td>\n",
       "      <td>1008141237</td>\n",
       "      <td>Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n",
       "      <td>201115110</td>\n",
       "      <td>938777978</td>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Peluche Donald - Europe - Disneyland 2000 (Mar...</td>\n",
       "      <td></td>\n",
       "      <td>50418756</td>\n",
       "      <td>457047496</td>\n",
       "      <td>Peluche Donald - Europe - Disneyland 2000 (Mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La Guerre Des Tuques</td>\n",
       "      <td>Luc a des id&amp;eacute;es de grandeur. Il veut or...</td>\n",
       "      <td>278535884</td>\n",
       "      <td>1077757786</td>\n",
       "      <td>La Guerre Des TuquesLuc a des id&amp;eacute;es de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84911</th>\n",
       "      <td>The Sims [ Import Anglais ]</td>\n",
       "      <td></td>\n",
       "      <td>206719094</td>\n",
       "      <td>941495734</td>\n",
       "      <td>The Sims [ Import Anglais ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84912</th>\n",
       "      <td>Kit piscine acier NEVADA déco pierre Ø 3.50m x...</td>\n",
       "      <td>&lt;b&gt;Description complète :&lt;/b&gt;&lt;br /&gt;Kit piscine...</td>\n",
       "      <td>3065095706</td>\n",
       "      <td>1188462883</td>\n",
       "      <td>Kit piscine acier NEVADA déco pierre Ø 3.50m x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84913</th>\n",
       "      <td>Journal Officiel De La Republique Francaise N°...</td>\n",
       "      <td></td>\n",
       "      <td>440707564</td>\n",
       "      <td>1009325617</td>\n",
       "      <td>Journal Officiel De La Republique Francaise N°...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84914</th>\n",
       "      <td>Table Basse Bois De Récupération Massif Base B...</td>\n",
       "      <td>&lt;p&gt;Cette table basse a un design unique et con...</td>\n",
       "      <td>3942400296</td>\n",
       "      <td>1267353403</td>\n",
       "      <td>Table Basse Bois De Récupération Massif Base B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84915</th>\n",
       "      <td>Gomme De Collection 2 Gommes Pinguin Glace Ver...</td>\n",
       "      <td></td>\n",
       "      <td>57203227</td>\n",
       "      <td>684671297</td>\n",
       "      <td>Gomme De Collection 2 Gommes Pinguin Glace Ver...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84916 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             designation  \\\n",
       "0      Olivia: Personalisiertes Notizbuch / 150 Seite...   \n",
       "1      Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...   \n",
       "2      Grand Stylet Ergonomique Bleu Gamepad Nintendo...   \n",
       "3      Peluche Donald - Europe - Disneyland 2000 (Mar...   \n",
       "4                                   La Guerre Des Tuques   \n",
       "...                                                  ...   \n",
       "84911                        The Sims [ Import Anglais ]   \n",
       "84912  Kit piscine acier NEVADA déco pierre Ø 3.50m x...   \n",
       "84913  Journal Officiel De La Republique Francaise N°...   \n",
       "84914  Table Basse Bois De Récupération Massif Base B...   \n",
       "84915  Gomme De Collection 2 Gommes Pinguin Glace Ver...   \n",
       "\n",
       "                                             description   productid  \\\n",
       "0                                                         3804725264   \n",
       "1                                                          436067568   \n",
       "2      PILOT STYLE Touch Pen de marque Speedlink est ...   201115110   \n",
       "3                                                           50418756   \n",
       "4      Luc a des id&eacute;es de grandeur. Il veut or...   278535884   \n",
       "...                                                  ...         ...   \n",
       "84911                                                      206719094   \n",
       "84912  <b>Description complète :</b><br />Kit piscine...  3065095706   \n",
       "84913                                                      440707564   \n",
       "84914  <p>Cette table basse a un design unique et con...  3942400296   \n",
       "84915                                                       57203227   \n",
       "\n",
       "          imageid                                               text  \n",
       "0      1263597046  Olivia: Personalisiertes Notizbuch / 150 Seite...  \n",
       "1      1008141237  Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...  \n",
       "2       938777978  Grand Stylet Ergonomique Bleu Gamepad Nintendo...  \n",
       "3       457047496  Peluche Donald - Europe - Disneyland 2000 (Mar...  \n",
       "4      1077757786  La Guerre Des TuquesLuc a des id&eacute;es de ...  \n",
       "...           ...                                                ...  \n",
       "84911   941495734                        The Sims [ Import Anglais ]  \n",
       "84912  1188462883  Kit piscine acier NEVADA déco pierre Ø 3.50m x...  \n",
       "84913  1009325617  Journal Officiel De La Republique Francaise N°...  \n",
       "84914  1267353403  Table Basse Bois De Récupération Massif Base B...  \n",
       "84915   684671297  Gomme De Collection 2 Gommes Pinguin Glace Ver...  \n",
       "\n",
       "[84916 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.fillna('',inplace=True)\n",
    "X['text'] = X.apply(lambda line: line['designation'] + line['description'],axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separer les données en train & text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60735</th>\n",
       "      <td>Carte Postale Typo Aimer - Kiub</td>\n",
       "      <td>Carte postale tendance de la collection Typo d...</td>\n",
       "      <td>2825941333</td>\n",
       "      <td>1208783386</td>\n",
       "      <td>Carte Postale Typo Aimer - KiubCarte postale t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9118</th>\n",
       "      <td>Garçon - Le Jeu De Plateau !</td>\n",
       "      <td>A propos : Il s¿agit d¿un jeu de cartes dans l...</td>\n",
       "      <td>89102802</td>\n",
       "      <td>856119038</td>\n",
       "      <td>Garçon - Le Jeu De Plateau !A propos : Il s¿ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55855</th>\n",
       "      <td>Royaume Des Animaux Ab À Asc</td>\n",
       "      <td></td>\n",
       "      <td>197015072</td>\n",
       "      <td>936925976</td>\n",
       "      <td>Royaume Des Animaux Ab À Asc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42138</th>\n",
       "      <td>Piscine Jeu Adresse 237x152</td>\n",
       "      <td>0</td>\n",
       "      <td>2824252365</td>\n",
       "      <td>1166755995</td>\n",
       "      <td>Piscine Jeu Adresse 237x1520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10948</th>\n",
       "      <td>United States And European Union Auditor Indep...</td>\n",
       "      <td></td>\n",
       "      <td>418466190</td>\n",
       "      <td>1017775450</td>\n",
       "      <td>United States And European Union Auditor Indep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6265</th>\n",
       "      <td>Griottines - Coffret De 5 Cl</td>\n",
       "      <td>Griottines - Coffret de 5 cl Cerises Griottes ...</td>\n",
       "      <td>183256510</td>\n",
       "      <td>938718607</td>\n",
       "      <td>Griottines - Coffret De 5 ClGriottines - Coffr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54886</th>\n",
       "      <td>1bouquet 7 Têtes Artificielles Pivoine Soie Fe...</td>\n",
       "      <td>1Bouquet 7 têtes artificielles Pivoine Soie Fe...</td>\n",
       "      <td>4086048240</td>\n",
       "      <td>1289079205</td>\n",
       "      <td>1bouquet 7 Têtes Artificielles Pivoine Soie Fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76820</th>\n",
       "      <td>Happy Easter Taies D'oreiller En Lin Coussin S...</td>\n",
       "      <td>Happy Easter taies d&amp;#39;oreiller en lin Couss...</td>\n",
       "      <td>3992621069</td>\n",
       "      <td>1273354395</td>\n",
       "      <td>Happy Easter Taies D'oreiller En Lin Coussin S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>Pratique Chasse Camo Camping Imperméable Camou...</td>\n",
       "      <td>Pratique Camo chasse furtif étanche Camping Ca...</td>\n",
       "      <td>4231695757</td>\n",
       "      <td>1323296971</td>\n",
       "      <td>Pratique Chasse Camo Camping Imperméable Camou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>Cine Live - N° 51 - Wasabi Reno</td>\n",
       "      <td>Le magazine qui se lit se regarde et s'écoute....</td>\n",
       "      <td>131689220</td>\n",
       "      <td>885908975</td>\n",
       "      <td>Cine Live - N° 51 - Wasabi RenoLe magazine qui...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67932 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             designation  \\\n",
       "60735                    Carte Postale Typo Aimer - Kiub   \n",
       "9118                        Garçon - Le Jeu De Plateau !   \n",
       "55855                       Royaume Des Animaux Ab À Asc   \n",
       "42138                        Piscine Jeu Adresse 237x152   \n",
       "10948  United States And European Union Auditor Indep...   \n",
       "...                                                  ...   \n",
       "6265                        Griottines - Coffret De 5 Cl   \n",
       "54886  1bouquet 7 Têtes Artificielles Pivoine Soie Fe...   \n",
       "76820  Happy Easter Taies D'oreiller En Lin Coussin S...   \n",
       "860    Pratique Chasse Camo Camping Imperméable Camou...   \n",
       "15795                    Cine Live - N° 51 - Wasabi Reno   \n",
       "\n",
       "                                             description   productid  \\\n",
       "60735  Carte postale tendance de la collection Typo d...  2825941333   \n",
       "9118   A propos : Il s¿agit d¿un jeu de cartes dans l...    89102802   \n",
       "55855                                                      197015072   \n",
       "42138                                                  0  2824252365   \n",
       "10948                                                      418466190   \n",
       "...                                                  ...         ...   \n",
       "6265   Griottines - Coffret de 5 cl Cerises Griottes ...   183256510   \n",
       "54886  1Bouquet 7 têtes artificielles Pivoine Soie Fe...  4086048240   \n",
       "76820  Happy Easter taies d&#39;oreiller en lin Couss...  3992621069   \n",
       "860    Pratique Camo chasse furtif étanche Camping Ca...  4231695757   \n",
       "15795  Le magazine qui se lit se regarde et s'écoute....   131689220   \n",
       "\n",
       "          imageid                                               text  \n",
       "60735  1208783386  Carte Postale Typo Aimer - KiubCarte postale t...  \n",
       "9118    856119038  Garçon - Le Jeu De Plateau !A propos : Il s¿ag...  \n",
       "55855   936925976                       Royaume Des Animaux Ab À Asc  \n",
       "42138  1166755995                       Piscine Jeu Adresse 237x1520  \n",
       "10948  1017775450  United States And European Union Auditor Indep...  \n",
       "...           ...                                                ...  \n",
       "6265    938718607  Griottines - Coffret De 5 ClGriottines - Coffr...  \n",
       "54886  1289079205  1bouquet 7 Têtes Artificielles Pivoine Soie Fe...  \n",
       "76820  1273354395  Happy Easter Taies D'oreiller En Lin Coussin S...  \n",
       "860    1323296971  Pratique Chasse Camo Camping Imperméable Camou...  \n",
       "15795   885908975  Cine Live - N° 51 - Wasabi RenoLe magazine qui...  \n",
       "\n",
       "[67932 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importer la classe train_test \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Séparer le jeu de données en données d'entraînement et données test \n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(X,y.prdtypecode, test_size=0.2,random_state=42)\n",
    "\n",
    "X_train_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokeniser: texte -> sequence entier (index dans un dictionaire):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Définition du tokenizer\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=50000)\n",
    "# Mettre à jour le dictionnaire du tokenizer\n",
    "tokenizer.fit_on_texts(X_train_text.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Transformer chaque review X_text_train en une séquence d'entiers à l'aide de la méthode texts_to_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train_text.text)\n",
    "X_test = tokenizer.texts_to_sequences(X_test_text.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stocker le dictionnaire de correspondance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des dictionnaires\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = tokenizer.index_word\n",
    "vocab_size = tokenizer.num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Transfomer la liste de sequence X_train en tableau numpy à l'aide de la fonction pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 500\n",
    "X_train_txt = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen, padding='post')\n",
    "X_test_txt = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  198, 10698, 40528, ...,     0,     0,     0],\n",
       "       [ 2492,    10,    70, ...,     0,     0,     0],\n",
       "       [ 3611,    17,   461, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [ 1802,  7702,  1366, ...,     0,     0,     0],\n",
       "       [  268,  3140,  6942, ...,     0,     0,     0],\n",
       "       [15137,  5659,   219, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open('classes.json') as f:\n",
    "    categories = json.load(f)\n",
    "\n",
    "i = 0\n",
    "y_dict = {}\n",
    "for category in categories:\n",
    "    y_train = y_train.replace(category,categories[category])\n",
    "    y_test = y_test.replace(category,categories[category])\n",
    "    y_dict[i] = category\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de notre jeu de données\n",
    "text_train_set = tf.data.Dataset.from_tensor_slices((X_train_txt, y_train.values))\n",
    "\n",
    "text_test_set = tf.data.Dataset.from_tensor_slices((X_test_txt, y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter la fonction load_image dans le pipeline des opérations. Séparer le résultat en lot de taille 32.\n",
    "text_train_set = text_train_set.map(lambda text, y: [text, y]).batch(32).repeat(-1)\n",
    "#dataset = dataset.map(lambda x, y: [load_image(x), y[:-1], y[1:]]).batch(16).repeat(-1)\n",
    "\n",
    "text_test_set = text_test_set.map(lambda text, y: [text, y]).batch(32).repeat(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modele pour classification de texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charger model déjà existent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'RNN'\n",
    "version = 'v4'\n",
    "model_name = model_type + '_' +  version\n",
    "model_path = 'models_output\\\\' + model_type + '\\\\' + version + '\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, GlobalAveragePooling1D, RNN, GRU, Dense,Dropout\n",
    "\n",
    "embedding_dim = 256\n",
    "voc_size_inp = len(tokenizer.word_counts)+1\n",
    "\n",
    "text_inputs = Input(shape=(None,), dtype='int32',name='input')\n",
    "x = Embedding(voc_size_inp, embedding_dim, name= 'embed_' + model_name)(text_inputs)\n",
    "#x = GRU(128, return_sequences=True, name='gru_' + model_name)(x)\n",
    "x = GRU(128,return_sequences=True, name = 'GRU_' + model_name)(x)\n",
    "#x = Dense(1024, activation='relu', name='dense_1_' + model_name)(x)\n",
    "x = Dropout(0.3, name='dropout_1' + model_name)(x)\n",
    "x = GlobalAveragePooling1D(name='batchnorm' + model_name)(x)\n",
    "RNN_v4 = Dense(256, activation='relu', name='dense_2_' + model_name)(x)\n",
    "#x = Dropout(0.3, name='dropout_2_' + model_name)(x)\n",
    "#text_output = Dense(27, activation='softmax', name='output_' + model_name)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KerasTensor' object has no attribute 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-d5fd8373f2f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcopy_old_models_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN_v4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-30-5b110fb4a03b>\u001b[0m in \u001b[0;36mcopy_old_models_weights\u001b[1;34m(model, old_model_name, model_path)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mold_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mnew_layer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mnew_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mold_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"      - {new_layer.name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KerasTensor' object has no attribute 'layers'"
     ]
    }
   ],
   "source": [
    "copy_old_models_weights(RNN_v4,model_name,model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_type_spec': TensorSpec(shape=(None, 128), dtype=tf.float32, name=None),\n",
       " '_inferred_value': None,\n",
       " '_name': 'batchnormRNN_v4/Mean:0',\n",
       " '_keras_history': KerasHistory(layer=<tensorflow.python.keras.layers.pooling.GlobalAveragePooling1D object at 0x00000288BCC9AFD0>, node_index=0, tensor_index=0)}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT TRAITÉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Données textuelles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = pd.read_csv(r'C:\\Users\\Edgar\\Documents\\Rakuten\\X_train_update.csv',index_col =0)\n",
    "X_treat = pd.read_csv(r'C:\\Users\\Edgar\\Documents\\Rakuten\\X_train\\X_train_lemma-FR_stop_words-FR_no_num-FR_remove_accents-FR_no_special-FR_lemma-EN_stop_words-EN_stop_words-DE_lemma-DE_steem-FR_steem-EN_steem-DE.csv',index_col =0)\n",
    "y = pd.read_csv(r'C:\\Users\\Edgar\\Documents\\Rakuten\\Y_train_CVw08PX.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre de mots par texte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_treat.rename(columns={'0':'text'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separer les données en train & text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>count_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60735</th>\n",
       "      <td>cart postal typo aim kiub cart postal tendanc ...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9118</th>\n",
       "      <td>garcon jeu plateau avoir propo agit un jeu car...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55855</th>\n",
       "      <td>royaum anim ab asc nan</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42138</th>\n",
       "      <td>piscin jeu adress x</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10948</th>\n",
       "      <td>unit stat european union auditor indep regul nan</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76233</th>\n",
       "      <td>piscin bou ballon gonflabl intex set complet p...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47141</th>\n",
       "      <td>pomp avoir chaleur pacfirst nov invert kw mono...</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42108</th>\n",
       "      <td>robocop play art kai figurin robocop cm avoir ...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72085</th>\n",
       "      <td>urss avoir heur k nan</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12667</th>\n",
       "      <td>special edit bronz sensat barb doll sp...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33966 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  count_word\n",
       "60735  cart postal typo aim kiub cart postal tendanc ...          38\n",
       "9118   garcon jeu plateau avoir propo agit un jeu car...          61\n",
       "55855                            royaum anim ab asc nan            6\n",
       "42138                               piscin jeu adress x            5\n",
       "10948  unit stat european union auditor indep regul nan            9\n",
       "...                                                  ...         ...\n",
       "76233  piscin bou ballon gonflabl intex set complet p...          61\n",
       "47141  pomp avoir chaleur pacfirst nov invert kw mono...         178\n",
       "42108  robocop play art kai figurin robocop cm avoir ...          31\n",
       "72085                             urss avoir heur k nan            6\n",
       "12667          special edit bronz sensat barb doll sp...          21\n",
       "\n",
       "[33966 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importer la classe train_test \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Séparer le jeu de données en données d'entraînement et données test \n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(X_treat,y.prdtypecode, test_size=0.2,random_state=42)\n",
    "\n",
    "X_train_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokeniser: texte -> sequence entier (index dans un dictionaire):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Définition du tokenizer\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=50000)\n",
    "# Mettre à jour le dictionnaire du tokenizer\n",
    "tokenizer.fit_on_texts(X_train_text.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Transformer chaque review X_text_train en une séquence d'entiers à l'aide de la méthode texts_to_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train_text.text)\n",
    "X_test = tokenizer.texts_to_sequences(X_test_text.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stocker le dictionnaire de correspondance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des dictionnaires\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = tokenizer.index_word\n",
    "vocab_size = tokenizer.num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Transfomer la liste de sequence X_train en tableau numpy à l'aide de la fonction pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 500\n",
    "X_train_txt = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen, padding='post')\n",
    "X_test_txt = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 121, 2575,  634, ...,    0,    0,    0],\n",
       "       [ 651,   32,  714, ...,    0,    0,    0],\n",
       "       [2643,  194,  413, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 876,  340, 5968, ...,    0,    0,    0],\n",
       "       [8793,   13,   35, ...,    0,    0,    0],\n",
       "       [ 291,  478, 3552, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open('classes.json') as f:\n",
    "    categories = json.load(f)\n",
    "\n",
    "i = 0\n",
    "y_dict = {}\n",
    "for category in categories:\n",
    "    y_train = y_train.replace(category,categories[category])\n",
    "    y_test = y_test.replace(category,categories[category])\n",
    "    y_dict[i] = category\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de notre jeu de données\n",
    "text_train_set = tf.data.Dataset.from_tensor_slices((X_train_txt, y_train.values))\n",
    "\n",
    "text_test_set = tf.data.Dataset.from_tensor_slices((X_test_txt, y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter la fonction load_image dans le pipeline des opérations. Séparer le résultat en lot de taille 32.\n",
    "text_train_set = text_train_set.map(lambda text, y: [text, y]).batch(32).repeat(-1)\n",
    "#dataset = dataset.map(lambda x, y: [load_image(x), y[:-1], y[1:]]).batch(16).repeat(-1)\n",
    "\n",
    "text_test_set = text_test_set.map(lambda text, y: [text, y]).batch(32).repeat(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modele pour classification de texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, GlobalAveragePooling1D, RNN, GRU, Dense,Dropout\n",
    "\n",
    "embedding_dim = 256\n",
    "voc_size_inp = len(tokenizer.word_counts)+1\n",
    "\n",
    "text_inputs = Input(shape=(None,), dtype='int32',name='input')\n",
    "x = Embedding(voc_size_inp, embedding_dim, name= 'embed_' + filename)(text_inputs)\n",
    "#x = GRU(128, return_sequences=True, name='gru_' + filename)(x)\n",
    "x = GRU(128,return_sequences=True, name = 'GRU_' + filename)(x)\n",
    "#x = Dense(1024, activation='relu', name='dense_1_' + filename)(x)\n",
    "x = Dropout(0.3, name='dropout_1' + filename)(x)\n",
    "x = GlobalAveragePooling1D(name='batchnorm' + filename)(x)\n",
    "x = Dense(256, activation='relu', name='dense_2_' + filename)(x)\n",
    "text_output = Dropout(0.3, name='dropout_2_' + filename)(x)\n",
    "#text_output = Dense(27, activation='softmax', name='output_' + filename)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMAGES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuperer les données images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_1263597046_product_3804725264.jpg\n",
      "C:\\Users\\Edgar\\Documents\\Rakuten\\images\\image_train\\image_1263597046_product_3804725264.jpg\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv('X_train_update.csv',index_col=0)\n",
    "y = pd.read_csv('Y_train_CVw08PX.csv',index_col=0).squeeze().map(str)\n",
    "\n",
    "#Create a column with the name of the picture\n",
    "X['image_name'] = 'image_' + X['imageid'].map(str) + '_product_' + X['productid'].map(str) + '.jpg'\n",
    "X['image_path'] = path + r'\\image_' + X['imageid'].map(str) + '_product_' + X['productid'].map(str) + '.jpg'\n",
    "print(X['image_name'].loc[0])\n",
    "print(X['image_path'].loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatener X_train et les labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>image_name</th>\n",
       "      <th>image_path</th>\n",
       "      <th>prdtypecode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "      <td>image_1263597046_product_3804725264.jpg</td>\n",
       "      <td>C:\\Users\\Edgar\\Documents\\Rakuten\\images\\image_...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>436067568</td>\n",
       "      <td>1008141237</td>\n",
       "      <td>image_1008141237_product_436067568.jpg</td>\n",
       "      <td>C:\\Users\\Edgar\\Documents\\Rakuten\\images\\image_...</td>\n",
       "      <td>2280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n",
       "      <td>201115110</td>\n",
       "      <td>938777978</td>\n",
       "      <td>image_938777978_product_201115110.jpg</td>\n",
       "      <td>C:\\Users\\Edgar\\Documents\\Rakuten\\images\\image_...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Peluche Donald - Europe - Disneyland 2000 (Mar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50418756</td>\n",
       "      <td>457047496</td>\n",
       "      <td>image_457047496_product_50418756.jpg</td>\n",
       "      <td>C:\\Users\\Edgar\\Documents\\Rakuten\\images\\image_...</td>\n",
       "      <td>1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La Guerre Des Tuques</td>\n",
       "      <td>Luc a des id&amp;eacute;es de grandeur. Il veut or...</td>\n",
       "      <td>278535884</td>\n",
       "      <td>1077757786</td>\n",
       "      <td>image_1077757786_product_278535884.jpg</td>\n",
       "      <td>C:\\Users\\Edgar\\Documents\\Rakuten\\images\\image_...</td>\n",
       "      <td>2705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84911</th>\n",
       "      <td>The Sims [ Import Anglais ]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>206719094</td>\n",
       "      <td>941495734</td>\n",
       "      <td>image_941495734_product_206719094.jpg</td>\n",
       "      <td>C:\\Users\\Edgar\\Documents\\Rakuten\\images\\image_...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84912</th>\n",
       "      <td>Kit piscine acier NEVADA déco pierre Ø 3.50m x...</td>\n",
       "      <td>&lt;b&gt;Description complète :&lt;/b&gt;&lt;br /&gt;Kit piscine...</td>\n",
       "      <td>3065095706</td>\n",
       "      <td>1188462883</td>\n",
       "      <td>image_1188462883_product_3065095706.jpg</td>\n",
       "      <td>C:\\Users\\Edgar\\Documents\\Rakuten\\images\\image_...</td>\n",
       "      <td>2583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84913</th>\n",
       "      <td>Journal Officiel De La Republique Francaise N°...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>440707564</td>\n",
       "      <td>1009325617</td>\n",
       "      <td>image_1009325617_product_440707564.jpg</td>\n",
       "      <td>C:\\Users\\Edgar\\Documents\\Rakuten\\images\\image_...</td>\n",
       "      <td>2280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84914</th>\n",
       "      <td>Table Basse Bois De Récupération Massif Base B...</td>\n",
       "      <td>&lt;p&gt;Cette table basse a un design unique et con...</td>\n",
       "      <td>3942400296</td>\n",
       "      <td>1267353403</td>\n",
       "      <td>image_1267353403_product_3942400296.jpg</td>\n",
       "      <td>C:\\Users\\Edgar\\Documents\\Rakuten\\images\\image_...</td>\n",
       "      <td>1560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84915</th>\n",
       "      <td>Gomme De Collection 2 Gommes Pinguin Glace Ver...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57203227</td>\n",
       "      <td>684671297</td>\n",
       "      <td>image_684671297_product_57203227.jpg</td>\n",
       "      <td>C:\\Users\\Edgar\\Documents\\Rakuten\\images\\image_...</td>\n",
       "      <td>2522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84916 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             designation  \\\n",
       "0      Olivia: Personalisiertes Notizbuch / 150 Seite...   \n",
       "1      Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...   \n",
       "2      Grand Stylet Ergonomique Bleu Gamepad Nintendo...   \n",
       "3      Peluche Donald - Europe - Disneyland 2000 (Mar...   \n",
       "4                                   La Guerre Des Tuques   \n",
       "...                                                  ...   \n",
       "84911                        The Sims [ Import Anglais ]   \n",
       "84912  Kit piscine acier NEVADA déco pierre Ø 3.50m x...   \n",
       "84913  Journal Officiel De La Republique Francaise N°...   \n",
       "84914  Table Basse Bois De Récupération Massif Base B...   \n",
       "84915  Gomme De Collection 2 Gommes Pinguin Glace Ver...   \n",
       "\n",
       "                                             description   productid  \\\n",
       "0                                                    NaN  3804725264   \n",
       "1                                                    NaN   436067568   \n",
       "2      PILOT STYLE Touch Pen de marque Speedlink est ...   201115110   \n",
       "3                                                    NaN    50418756   \n",
       "4      Luc a des id&eacute;es de grandeur. Il veut or...   278535884   \n",
       "...                                                  ...         ...   \n",
       "84911                                                NaN   206719094   \n",
       "84912  <b>Description complète :</b><br />Kit piscine...  3065095706   \n",
       "84913                                                NaN   440707564   \n",
       "84914  <p>Cette table basse a un design unique et con...  3942400296   \n",
       "84915                                                NaN    57203227   \n",
       "\n",
       "          imageid                               image_name  \\\n",
       "0      1263597046  image_1263597046_product_3804725264.jpg   \n",
       "1      1008141237   image_1008141237_product_436067568.jpg   \n",
       "2       938777978    image_938777978_product_201115110.jpg   \n",
       "3       457047496     image_457047496_product_50418756.jpg   \n",
       "4      1077757786   image_1077757786_product_278535884.jpg   \n",
       "...           ...                                      ...   \n",
       "84911   941495734    image_941495734_product_206719094.jpg   \n",
       "84912  1188462883  image_1188462883_product_3065095706.jpg   \n",
       "84913  1009325617   image_1009325617_product_440707564.jpg   \n",
       "84914  1267353403  image_1267353403_product_3942400296.jpg   \n",
       "84915   684671297     image_684671297_product_57203227.jpg   \n",
       "\n",
       "                                              image_path prdtypecode  \n",
       "0      C:\\Users\\Edgar\\Documents\\Rakuten\\images\\image_...          10  \n",
       "1      C:\\Users\\Edgar\\Documents\\Rakuten\\images\\image_...        2280  \n",
       "2      C:\\Users\\Edgar\\Documents\\Rakuten\\images\\image_...          50  \n",
       "3      C:\\Users\\Edgar\\Documents\\Rakuten\\images\\image_...        1280  \n",
       "4      C:\\Users\\Edgar\\Documents\\Rakuten\\images\\image_...        2705  \n",
       "...                                                  ...         ...  \n",
       "84911  C:\\Users\\Edgar\\Documents\\Rakuten\\images\\image_...          40  \n",
       "84912  C:\\Users\\Edgar\\Documents\\Rakuten\\images\\image_...        2583  \n",
       "84913  C:\\Users\\Edgar\\Documents\\Rakuten\\images\\image_...        2280  \n",
       "84914  C:\\Users\\Edgar\\Documents\\Rakuten\\images\\image_...        1560  \n",
       "84915  C:\\Users\\Edgar\\Documents\\Rakuten\\images\\image_...        2522  \n",
       "\n",
       "[84916 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.concat([X,y],axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois nous avons un dataset de la taille desirée on peut le séparer en train et test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_img, X_test_img, y_train, y_test = train_test_split(X[['image_name','prdtypecode']], X.prdtypecode, train_size = 0.4 ,test_size=0.2, random_state=42)\n",
    "#X_train_path, X_test_path, y_train, y_test = train_test_split(X.image_path, X.label, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>prdtypecode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60735</th>\n",
       "      <td>image_1208783386_product_2825941333.jpg</td>\n",
       "      <td>1320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9118</th>\n",
       "      <td>image_856119038_product_89102802.jpg</td>\n",
       "      <td>1281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55855</th>\n",
       "      <td>image_936925976_product_197015072.jpg</td>\n",
       "      <td>2403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42138</th>\n",
       "      <td>image_1166755995_product_2824252365.jpg</td>\n",
       "      <td>1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10948</th>\n",
       "      <td>image_1017775450_product_418466190.jpg</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76233</th>\n",
       "      <td>image_1075701044_product_1351876762.jpg</td>\n",
       "      <td>2583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47141</th>\n",
       "      <td>image_1174586088_product_2940638611.jpg</td>\n",
       "      <td>2583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42108</th>\n",
       "      <td>image_984906419_product_290034883.jpg</td>\n",
       "      <td>1140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72085</th>\n",
       "      <td>image_901400003_product_62981761.jpg</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12667</th>\n",
       "      <td>image_1215506278_product_3470052832.jpg</td>\n",
       "      <td>1280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33966 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    image_name prdtypecode\n",
       "60735  image_1208783386_product_2825941333.jpg        1320\n",
       "9118      image_856119038_product_89102802.jpg        1281\n",
       "55855    image_936925976_product_197015072.jpg        2403\n",
       "42138  image_1166755995_product_2824252365.jpg        1302\n",
       "10948   image_1017775450_product_418466190.jpg          10\n",
       "...                                        ...         ...\n",
       "76233  image_1075701044_product_1351876762.jpg        2583\n",
       "47141  image_1174586088_product_2940638611.jpg        2583\n",
       "42108    image_984906419_product_290034883.jpg        1140\n",
       "72085     image_901400003_product_62981761.jpg          10\n",
       "12667  image_1215506278_product_3470052832.jpg        1280\n",
       "\n",
       "[33966 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELE POUR CLASSIFICATION D'IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "\n",
    "base_model = VGG16(weights='imagenet',input_shape=(224, 224, 3),include_top=False)\n",
    "\n",
    "image_input = Input(shape=(224, 224, 3), name='image')\n",
    "x = base_model(image_input)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "images = Dropout(0.2)(x)\n",
    "#x = Flatten()(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generateurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation des données:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 33966 validated image filenames belonging to 27 classes.\n",
      "Found 16984 validated image filenames belonging to 27 classes.\n"
     ]
    }
   ],
   "source": [
    "#APPLY SOME TRANSFORMATIONS TO DATA\n",
    "\n",
    "\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "image_train_set = train_datagen.flow_from_dataframe(dataframe=X_train_img,\n",
    "                                              directory=path,\n",
    "                                              x_col = \"image_name\",\n",
    "                                              y_col = \"prdtypecode\",\n",
    "                                              class_mode =\"sparse\",\n",
    "                                              target_size = (224, 224),\n",
    "                                              batch_size = 32)\n",
    "\n",
    "image_test_set = test_datagen.flow_from_dataframe(dataframe=X_test_img,\n",
    "                                              directory=path,\n",
    "                                              x_col = \"image_name\",\n",
    "                                              y_col = \"prdtypecode\",\n",
    "                                              class_mode =\"sparse\",\n",
    "                                              target_size = (224, 224),\n",
    "                                              batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition d'un générateur python\n",
    "def generator(image_set,text_set):\n",
    "    iter_text = iter(text_set)\n",
    "    iter_image = iter(image_set)\n",
    "    while True:\n",
    "        X_text, y_text = next(iter_text) #do not use this 'y_text' from text iterator!!! Tf does not recognise it and leads to bug during training\n",
    "        X_im, y = next(iter_image)\n",
    "        \n",
    "        #print('')\n",
    "        #print('X_im:',X_im.shape,'X_text:',X_text.shape,'y:',y.shape)\n",
    "        #print(y)\n",
    "        yield [X_im, X_text], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_train_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-c4225e6f6e53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Définition du générateur final.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgen_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_train_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext_train_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgen_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_test_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext_test_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_train_set' is not defined"
     ]
    }
   ],
   "source": [
    "# Définition du générateur final.\n",
    "gen_train = generator(image_train_set,text_train_set)\n",
    "\n",
    "gen_test = generator(image_test_set,text_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCATENATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image (InputLayer)              [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vgg16 (Functional)              (None, 7, 7, 512)    14714688    image[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 512)          0           vgg16[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1024)         525312      global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 1024)         0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 512)          524800      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 256)    13705728    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 512)          0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 512)          1574912     embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1024)         0           dropout_7[0][0]                  \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 512)          524800      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 512)          0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 27)           13851       dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 31,584,091\n",
      "Trainable params: 16,869,403\n",
      "Non-trainable params: 14,714,688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#https://towardsdatascience.com/deep-multi-input-models-transfer-learning-for-image-and-word-tag-recognition-7ae0462253dc\n",
    "\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "x = concatenate([EffNetB1, RNN_v4, RNN_v3], axis=-1)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "output = Dense(27, activation='softmax')(x)\n",
    "\n",
    "model = Model([image_input, text_inputs, treated_text_inputs], output)\n",
    "\n",
    "unfreezed_layers = 0 #Nombre de couches a décongeler pour aplique le finetuning: Voir livre Deep Learning with python\n",
    "# Freezer les couches du VGG16\n",
    "for layer in base_model.layers[-unfreezed_layers:]: \n",
    "    layer.trainable = False\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                         patience=3,\n",
    "                                         mode='min',\n",
    "                                         restore_best_weights=True)\n",
    "\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "filepath = cwd + '/out'\n",
    "\n",
    "checkpoint = callbacks.ModelCheckpoint(filepath=filepath + '/concat_RNN_VGG16.hdf5', \n",
    "                                       monitor='val_loss',\n",
    "                                       save_best_only=True,\n",
    "                                       save_weights_only=False,\n",
    "                                       mode='min',\n",
    "                                       save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_im: (32, 224, 224, 3) X_text: (32, 500) y: (32,)\n",
      "[13.  4. 10. 25.  4.  1. 20. 10. 10. 18. 24. 11.  4. 18. 20. 11. 20. 10.\n",
      " 17.  0. 26. 18. 24.  0. 18.  0. 25.  9. 15. 15. 20. 11.]\n",
      "Epoch 1/10\n",
      "X_im: (32, 224, 224, 3) X_text: (32, 500) y: (32,)\n",
      "[ 5. 15. 23. 20. 13. 13. 16.  3. 22.  2. 10.  9.  1. 11. 22. 18.  6. 20.\n",
      "  9. 11.  9. 14. 17.  5. 16. 13. 19. 11. 20.  2. 20. 13.]\n",
      "   1/1061 [..............................] - ETA: 3:09:09 - loss: 3.2603 - accuracy: 0.0625X_im: (32, 224, 224, 3) X_text: (32, 500) y: (32,)\n",
      "[15.  8. 22. 20. 17. 20. 20. 10. 10. 25. 17. 13.  1. 16. 10.  6.  2. 21.\n",
      "  0.  6. 16.  0. 18.  7.  0. 16. 22. 20. 23. 11. 13. 24.]\n",
      "   2/1061 [..............................] - ETA: 2:28:55 - loss: 3.3050 - accuracy: 0.0938X_im: (32, 224, 224, 3) X_text: (32, 500) y: (32,)\n",
      "[ 5.  4. 21. 20. 13. 16. 15. 19. 25. 19. 10.  2. 16.  9. 10. 22.  6. 11.\n",
      " 11. 13. 20.  2. 24.  6. 10.  9. 20. 20. 20. 20. 20.  9.]\n",
      "   3/1061 [..............................] - ETA: 2:28:17 - loss: 3.2978 - accuracy: 0.0833X_im: (32, 224, 224, 3) X_text: (32, 500) y: (32,)\n",
      "[ 2.  4. 20. 24.  0. 13. 20. 16.  0. 23.  2.  6. 25. 11. 20. 18. 20. 21.\n",
      " 13. 13.  4. 18. 13. 16. 25.  2.  4.  9. 21.  6. 18. 16.]\n",
      "   4/1061 [..............................] - ETA: 2:28:42 - loss: 3.2566 - accuracy: 0.1016X_im: (32, 224, 224, 3) X_text: (32, 500) y: (32,)\n",
      "[ 0.  4. 24. 22.  8.  4. 16. 10. 18. 11. 13. 18. 23. 25. 10.  2.  5. 19.\n",
      "  1.  6.  9. 15.  3. 20.  4. 20. 19. 21.  2. 16.  0. 18.]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-8248bf9f0c62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#model.fit_generator(gen, steps_per_epoch=int(len(y_train.values)/16), validation_data = gentest, validation_steps = int(len(y_test.values)/16),epochs=10, workers=-1,callbacks=[early_stopping, checkpoint])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#https://www.geeksforgeeks.org/keras-fit-and-keras-fit_generator/\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#model.fit_generator(gen, steps_per_epoch=int(len(y_train.values)/16), validation_data = gentest, validation_steps = int(len(y_test.values)/16),epochs=10, workers=-1,callbacks=[early_stopping, checkpoint])\n",
    "model.fit(gen_train, steps_per_epoch=int(len(y_train.values)/32), validation_data = gen_test, validation_steps = int(len(y_test.values)/32),epochs=10, workers=1)\n",
    "\n",
    "\n",
    "#https://www.geeksforgeeks.org/keras-fit-and-keras-fit_generator/\n",
    "#So, we have learned the difference between Keras.fit and Keras.fit_generator functions used to train a deep learning neural network\n",
    "#.fit is used when the entire training dataset can fit into the memory and no data augmentation is applied.\n",
    "#.fit_generator is used when either we have a huge dataset to fit into our memory or when data augmentation needs to be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokeniser: texte -> sequence entier (index dans un dictionaire):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Définition du tokenizer\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)\n",
    "# Mettre à jour le dictionnaire du tokenizer\n",
    "tokenizer.fit_on_texts(X_train_text.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Transformer chaque review X_text_train en une séquence d'entiers à l'aide de la méthode texts_to_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train_text.text)\n",
    "X_test = tokenizer.texts_to_sequences(X_test_text.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stocker le dictionnaire de correspondance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des dictionnaires\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = tokenizer.index_word\n",
    "vocab_size = tokenizer.num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Transfomer la liste de sequence X_train en tableau numpy à l'aide de la fonction pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 500\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen, padding='post')\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
