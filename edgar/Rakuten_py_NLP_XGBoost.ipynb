{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rakuten_py_NLP_XGBoost.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM7EZ6WohN0V307Lnxe6+lO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ragdehl/Rakuten_py/blob/main/edgar/Rakuten_py_NLP_XGBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRPb0zpgpNln"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QopqlyDyqX6U"
      },
      "source": [
        "Récuperer les données:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMv27FPIqQwp",
        "outputId": "a352d823-9e69-4eb0-f778-d2c542048690"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DP6gQD43qI2r",
        "outputId": "3a76b93e-8d70-4e82-f47c-961e952ca4f5"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb4v2A1npf--"
      },
      "source": [
        "# Initialiser la variable des mots vides\n",
        "stop_words = set(stopwords.words('french'))\n",
        " \n",
        "df_X = pd.read_csv('/content/drive/My Drive/Rakuten/X_train_update.csv',index_col=0)\n",
        "df_y = pd.read_csv('/content/drive/My Drive/Rakuten/Y_train_CVw08PX.csv',index_col=0)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QLRSPSTpj3U"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemma(sentence): #Lemmatizer\n",
        "    doc = word_tokenize(sentence, language='french')\n",
        "    return [lemmatizer.lemmatize(token) for token in doc]\n",
        "\n",
        "def stop_words_filetring(mots) : \n",
        "    tokens = []\n",
        "    for mot in mots:\n",
        "        if mot not in stop_words:\n",
        "            tokens.append(mot)\n",
        "    return tokens\n",
        "\n",
        "def clean_text(text):\n",
        "    string = ''\n",
        "    words = word_tokenize(text.lower(), language='french')\n",
        "    for word in words:\n",
        "        if word not in stop_words:\n",
        "            #if word.isascii(): #and word.isalpha():\n",
        "            string += lemmatizer.lemmatize(word) + ' '\n",
        "    return string"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2aFUFWVrzZe"
      },
      "source": [
        "On netoye un peu le texte:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFKw8sl2rxxi",
        "outputId": "8efd6f7d-2b0c-48d7-8de6-b873ddb8f13a"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20vm3aMzpmbj",
        "outputId": "151a600a-fcf5-41db-8751-a4b1c7abff46"
      },
      "source": [
        "X = df_X.designation.astype(str) + ' ' + df_X.description.astype(str)\n",
        "y = df_y.prdtypecode\n",
        "\n",
        "X_clean = X.apply(lambda cell: clean_text(cell))\n",
        "X_clean"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        olivia : personalisiertes notizbuch / 150 seit...\n",
              "1        journal art ( ) n° 133 28/09/2001 - l'art marc...\n",
              "2        grand stylet ergonomique bleu gamepad nintendo...\n",
              "3        peluche donald - europe - disneyland 2000 ( ma...\n",
              "4        guerre tuques luc a id & eacute ; grandeur . v...\n",
              "                               ...                        \n",
              "84911                     the sims [ import anglais ] nan \n",
              "84912    kit piscine acier nevada déco pierre ø 3.50m x...\n",
              "84913    journal officiel republique francaise n° 46 15...\n",
              "84914    table basse bois récupération massif base blan...\n",
              "84915    gomme collection 2 gommes pinguin glace vert o...\n",
              "Length: 84916, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHMVohjHuNlb"
      },
      "source": [
        "Combien de mots uniques?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO3Mi_k9uM4y",
        "outputId": "18d1a00f-6efe-462e-e6dd-0e0071cbb853"
      },
      "source": [
        "lis = []\n",
        "for element in X_clean.str.split():\n",
        "    for word in element:\n",
        "        lis.append(word)\n",
        "\n",
        "len(list(set(lis)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "235642"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HONvmBt7uWCP"
      },
      "source": [
        "Transformer les categories en numeros de 0 à 26:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzwnBnypuiFx",
        "outputId": "273d8064-439f-46f5-eb6d-e1cbd07d4dd3"
      },
      "source": [
        "categories = list(set(y.to_list()))\n",
        "y_trans = y\n",
        "i = 0\n",
        "for category in categories:\n",
        "    y_trans = y_trans.replace(category,i)\n",
        "    i+=1\n",
        "\n",
        "y_trans"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         4\n",
              "1        25\n",
              "2        20\n",
              "3         0\n",
              "4         6\n",
              "         ..\n",
              "84911    17\n",
              "84912    10\n",
              "84913    25\n",
              "84914    11\n",
              "84915    23\n",
              "Name: prdtypecode, Length: 84916, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpttDLYRuwjF"
      },
      "source": [
        "Separer en train et test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWm29VRzp73v"
      },
      "source": [
        "# Importer la classe train_test \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Séparer le jeu de données en données d'entraînement et données test \n",
        "X_train, X_test, y_train, y_test = train_test_split(X_clean, y_trans,train_size=0.1, test_size=0.02)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lijR_tXevnxO"
      },
      "source": [
        "Vectoriser avec TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CIRBjIHvoRL"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfid = TfidfVectorizer(analyzer='word',\n",
        "                  tokenizer=word_tokenize,\n",
        "                      #strip_accents='unicode',\n",
        "                      #stop_words=french_stop_words_no_accent, # peut etre interessant parce que lisse la progression\n",
        "                  max_df=0.8,\n",
        "                  min_df=2,\n",
        "                  ngram_range=(1,2),\n",
        "                  use_idf=True,\n",
        "                  smooth_idf=True,\n",
        "                  sublinear_tf=False,\n",
        "                  binary=True,\n",
        "                  )\n",
        "\n",
        "X_train_trans = tfid.fit_transform(X_train)\n",
        "X_test_trans = tfid.transform(X_test)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTtWBdc7xGHF"
      },
      "source": [
        "X_train_trans = X_train_trans.todense()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Alg5wyOIxHEk"
      },
      "source": [
        "X_test_trans = X_test_trans.todense()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0QSm7VmxMOs",
        "outputId": "63d8e2c3-625e-4744-d9bb-af6a9dd660ca"
      },
      "source": [
        "input_dim = X_train_trans.shape[1]\n",
        "input_dim"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "82925"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOTq7HQh3ANS"
      },
      "source": [
        "Vectoriser avec CountVectoriser:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQex8O363Aqa"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialiser un objet vectorisateur\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Mettre à jour la valeur de X_train et X_test\n",
        "X_train = vectorizer.fit_transform(X_train).todense()\n",
        "X_test = vectorizer.transform(X_test).todense()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp2kFcEivPd3"
      },
      "source": [
        "Utiliser modèle XGBoost:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5uDjtX2uL2D"
      },
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "train = xgb.DMatrix(X_train,y_train)\n",
        "test = xgb.DMatrix(X_test,y_test)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHo-Ad4m0xqC",
        "outputId": "48a00e28-18b0-4e36-fe09-e1524ccd2715"
      },
      "source": [
        "\n",
        "from datetime import datetime\n",
        "\n",
        "params = {'booster':'gbtree', 'learning_rate': 1,'objective':'multi:softmax','num_class':27}\n",
        "\n",
        "now = datetime.now()\n",
        "\n",
        "current_time = now.strftime(\"%H:%M:%S\")\n",
        "print(\"Start Time =\", current_time)\n",
        "\n",
        "xgb1 = xgb.train(params = params,\n",
        "                 dtrain = train,\n",
        "                 num_boost_round= 100,\n",
        "                 early_stopping_rounds= 15,\n",
        "                 evals= [(train, 'train'), (test, 'eval')])\n",
        "\n",
        "now = datetime.now()\n",
        "\n",
        "current_time = now.strftime(\"%H:%M:%S\")\n",
        "print(\"End Time =\", current_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Time = 22:29:19\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}